{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a79c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train full took 0.11234246799722314 sec\n",
      "Data Wrangling for train took 0.013182478956878185 sec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import Timer\n",
    "\n",
    "def cutomizedCoordinationFix(df):\n",
    "    df = df.assign(rev=df.dropoff_latitude<df.dropoff_longitude)\n",
    "    idx = (df['rev'] == 1)\n",
    "    df.loc[idx,['dropoff_longitude','dropoff_latitude']] = df.loc[idx,['dropoff_latitude','dropoff_longitude']].values\n",
    "    df.loc[idx,['pickup_longitude','pickup_latitude']] = df.loc[idx,['pickup_latitude','pickup_longitude']].values\n",
    "    df = df.drop(columns=['rev'])\n",
    "    return df\n",
    "\n",
    "def clean_df(df):    \n",
    "    #reverse incorrectly assigned longitude/latitude values\n",
    "    df = cutomizedCoordinationFix(df)\n",
    "    df = df[(df.fare_amount > 0)  & (df.fare_amount <= 500) &\n",
    "          (df.passenger_count >= 0) & (df.passenger_count <= 8)  &\n",
    "           ((df.pickup_longitude != 0) & (df.pickup_latitude != 0) & (df.dropoff_longitude != 0) & (df.dropoff_latitude != 0) )]\n",
    "    \n",
    "    return df\n",
    "\n",
    "cols = [\n",
    "    'fare_amount', 'pickup_datetime','pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'passenger_count'\n",
    "]\n",
    "\n",
    "#sampled_line = 100000\n",
    "with Timer(f\"Load train full\"):\n",
    "    train_data = pd.read_csv(\"nyc_taxi_fare_train.csv\", usecols=cols, nrows=100000)\n",
    "\n",
    "with Timer(\"Data Wrangling for train\"):\n",
    "    train_data = clean_df(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0c0813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2013-09-24 07:39:00 UTC</td>\n",
       "      <td>-73.947977</td>\n",
       "      <td>40.784792</td>\n",
       "      <td>-73.964262</td>\n",
       "      <td>40.792347</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2014-05-15 12:15:45 UTC</td>\n",
       "      <td>-73.962918</td>\n",
       "      <td>40.799107</td>\n",
       "      <td>-73.974178</td>\n",
       "      <td>40.786487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2015-02-19 17:40:43 UTC</td>\n",
       "      <td>-73.996773</td>\n",
       "      <td>40.723549</td>\n",
       "      <td>-73.991974</td>\n",
       "      <td>40.724724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>6.9</td>\n",
       "      <td>2009-10-10 23:35:00 UTC</td>\n",
       "      <td>-73.983652</td>\n",
       "      <td>40.756667</td>\n",
       "      <td>-73.982715</td>\n",
       "      <td>40.767067</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2010-11-09 16:09:00 UTC</td>\n",
       "      <td>-73.975663</td>\n",
       "      <td>40.791653</td>\n",
       "      <td>-73.982267</td>\n",
       "      <td>40.774968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97983 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "0              4.5  2009-06-15 17:26:21 UTC        -73.844311   \n",
       "1             16.9  2010-01-05 16:52:16 UTC        -74.016048   \n",
       "2              5.7  2011-08-18 00:35:00 UTC        -73.982738   \n",
       "3              7.7  2012-04-21 04:30:42 UTC        -73.987130   \n",
       "4              5.3  2010-03-09 07:51:00 UTC        -73.968095   \n",
       "...            ...                      ...               ...   \n",
       "99995          9.0  2013-09-24 07:39:00 UTC        -73.947977   \n",
       "99996          6.0  2014-05-15 12:15:45 UTC        -73.962918   \n",
       "99997          5.0  2015-02-19 17:40:43 UTC        -73.996773   \n",
       "99998          6.9  2009-10-10 23:35:00 UTC        -73.983652   \n",
       "99999          5.7  2010-11-09 16:09:00 UTC        -73.975663   \n",
       "\n",
       "       pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0            40.721319         -73.841610         40.712278                1  \n",
       "1            40.711303         -73.979268         40.782004                1  \n",
       "2            40.761270         -73.991242         40.750562                2  \n",
       "3            40.733143         -73.991567         40.758092                1  \n",
       "4            40.768008         -73.956655         40.783762                1  \n",
       "...                ...                ...               ...              ...  \n",
       "99995        40.784792         -73.964262         40.792347                5  \n",
       "99996        40.799107         -73.974178         40.786487                1  \n",
       "99997        40.723549         -73.991974         40.724724                1  \n",
       "99998        40.756667         -73.982715         40.767067                4  \n",
       "99999        40.791653         -73.982267         40.774968                1  \n",
       "\n",
       "[97983 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8a799f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20221201_194723/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20221201_194723/\"\n",
      "AutoGluon Version:  0.6.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Nov 8 23:39:32 UTC 2018\n",
      "Train Data Rows:    97983\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (200.0, 0.01, 11.3466, 9.68786)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    357330.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.76 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t16.0s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.84 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 16.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.025514630088892973, Train Rows: 95483, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-9.9861\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-10.9575\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.18268\n",
      "[2000]\tvalid_set's rmse: 4.14276\n",
      "[3000]\tvalid_set's rmse: 4.13572\n",
      "[4000]\tvalid_set's rmse: 4.13964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.1305\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.89882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.8981\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.98s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-4.367\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-3.9498\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.74s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-4.382\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-7.6499\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.04s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-3.9719\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-4.119\t = Validation score   (-root_mean_squared_error)\n",
      "\t581.09s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.89101\n",
      "[2000]\tvalid_set's rmse: 3.85401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.8453\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.38s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-3.8103\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 847.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20221201_194723/\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "model = TabularPredictor(label=\"fare_amount\")\n",
    "predictor = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc17ee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeightedEnsemble_L2',\n",
       " 'model_type': 'WeightedEnsembleModel',\n",
       " 'problem_type': 'regression',\n",
       " 'eval_metric': 'root_mean_squared_error',\n",
       " 'stopping_metric': 'root_mean_squared_error',\n",
       " 'fit_time': 0.2618563175201416,\n",
       " 'num_classes': None,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.00039458274841308594,\n",
       " 'val_score': -3.810278397873824,\n",
       " 'hyperparameters': {'use_orig_features': False,\n",
       "  'max_base_models': 25,\n",
       "  'max_base_models_per_type': 5,\n",
       "  'save_bag_folds': True},\n",
       " 'hyperparameters_fit': {},\n",
       " 'hyperparameters_nondefault': ['save_bag_folds'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'valid_raw_types': None,\n",
       "  'valid_special_types': None,\n",
       "  'ignored_type_group_special': None,\n",
       "  'ignored_type_group_raw': None,\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None,\n",
       "  'predict_1_batch_size': None,\n",
       "  'temperature_scalar': None,\n",
       "  'drop_unique': False},\n",
       " 'num_features': 5,\n",
       " 'features': ['LightGBM',\n",
       "  'NeuralNetFastAI',\n",
       "  'NeuralNetTorch',\n",
       "  'ExtraTreesMSE',\n",
       "  'LightGBMLarge'],\n",
       " 'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x7f0b7944e3a0>,\n",
       " 'memory_size': 4322,\n",
       " 'compile_time': None,\n",
       " 'bagged_info': {'child_model_type': 'GreedyWeightedEnsembleModel',\n",
       "  'num_child_models': 1,\n",
       "  'child_model_names': ['S1F1'],\n",
       "  '_n_repeats': 1,\n",
       "  '_k_per_n_repeat': [1],\n",
       "  '_random_state': 2,\n",
       "  'low_memory': False,\n",
       "  'bagged_mode': False,\n",
       "  'max_memory_size': 4322,\n",
       "  'min_memory_size': 4322,\n",
       "  'child_hyperparameters': {'ensemble_size': 100},\n",
       "  'child_hyperparameters_fit': {'ensemble_size': 91},\n",
       "  'child_ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "   'max_time_limit_ratio': 1.0,\n",
       "   'max_time_limit': None,\n",
       "   'min_time_limit': 0,\n",
       "   'valid_raw_types': None,\n",
       "   'valid_special_types': None,\n",
       "   'ignored_type_group_special': None,\n",
       "   'ignored_type_group_raw': None,\n",
       "   'get_features_kwargs': None,\n",
       "   'get_features_kwargs_extra': None,\n",
       "   'predict_1_batch_size': None,\n",
       "   'temperature_scalar': None,\n",
       "   'drop_unique': False}},\n",
       " 'stacker_info': {'num_base_models': 5,\n",
       "  'base_model_names': ['LightGBM',\n",
       "   'ExtraTreesMSE',\n",
       "   'NeuralNetFastAI',\n",
       "   'NeuralNetTorch',\n",
       "   'LightGBMLarge']},\n",
       " 'children_info': {'S1F1': {'name': 'S1F1',\n",
       "   'model_type': 'GreedyWeightedEnsembleModel',\n",
       "   'problem_type': 'regression',\n",
       "   'eval_metric': 'root_mean_squared_error',\n",
       "   'stopping_metric': 'root_mean_squared_error',\n",
       "   'fit_time': 0.2618563175201416,\n",
       "   'num_classes': None,\n",
       "   'quantile_levels': None,\n",
       "   'predict_time': None,\n",
       "   'val_score': None,\n",
       "   'hyperparameters': {'ensemble_size': 100},\n",
       "   'hyperparameters_fit': {'ensemble_size': 91},\n",
       "   'hyperparameters_nondefault': [],\n",
       "   'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "    'max_time_limit_ratio': 1.0,\n",
       "    'max_time_limit': None,\n",
       "    'min_time_limit': 0,\n",
       "    'valid_raw_types': None,\n",
       "    'valid_special_types': None,\n",
       "    'ignored_type_group_special': None,\n",
       "    'ignored_type_group_raw': None,\n",
       "    'get_features_kwargs': None,\n",
       "    'get_features_kwargs_extra': None,\n",
       "    'predict_1_batch_size': None,\n",
       "    'temperature_scalar': None,\n",
       "    'drop_unique': False},\n",
       "   'num_features': 5,\n",
       "   'features': ['LightGBM',\n",
       "    'ExtraTreesMSE',\n",
       "    'NeuralNetFastAI',\n",
       "    'NeuralNetTorch',\n",
       "    'LightGBMLarge'],\n",
       "   'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x7f0b7944e340>,\n",
       "   'memory_size': 9054,\n",
       "   'compile_time': None,\n",
       "   'model_weights': {'LightGBM': 0.2087912087912088,\n",
       "    'ExtraTreesMSE': 0.02197802197802198,\n",
       "    'NeuralNetFastAI': 0.02197802197802198,\n",
       "    'NeuralNetTorch': 0.14285714285714285,\n",
       "    'LightGBMLarge': 0.6043956043956044}}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = predictor.info()\n",
    "info['model_info'][info['best_model']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe07bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train full took 50.208256038837135 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20221201_210756/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangling for train took 7.748062551021576 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (54315955 samples, 7386.97 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20221201_210756/\"\n",
      "AutoGluon Version:  0.6.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Nov 8 23:39:32 UTC 2018\n",
      "Train Data Rows:    54315955\n",
      "Train Data Columns: 6\n",
      "Label Column: fare_amount\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (500.0, 0.01, 11.32425, 9.68662)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    342096.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6517.91 MB (1.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                        : 1 | ['passenger_count']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['pickup_datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "\t\t('int', [])                  : 1 | ['passenger_count']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['pickup_datetime', 'pickup_datetime.year', 'pickup_datetime.month', 'pickup_datetime.day', 'pickup_datetime.dayofweek']\n",
      "\t9415.7s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4345.28 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 9426.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 53772795, Val Rows: 543160\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-9.2637\t = Validation score   (-root_mean_squared_error)\n",
      "\t274.18s\t = Training   runtime\n",
      "\t5.54s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-9.0431\t = Validation score   (-root_mean_squared_error)\n",
      "\t260.24s\t = Training   runtime\n",
      "\t5.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 4.25444\n",
      "[2000]\tvalid_set's rmse: 4.14332\n",
      "[3000]\tvalid_set's rmse: 4.09463\n",
      "[4000]\tvalid_set's rmse: 4.06868\n",
      "[5000]\tvalid_set's rmse: 4.04759\n",
      "[6000]\tvalid_set's rmse: 4.03167\n",
      "[7000]\tvalid_set's rmse: 4.01966\n",
      "[8000]\tvalid_set's rmse: 4.00981\n",
      "[9000]\tvalid_set's rmse: 4.00126\n",
      "[10000]\tvalid_set's rmse: 3.99384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.9938\t = Validation score   (-root_mean_squared_error)\n",
      "\t3826.13s\t = Training   runtime\n",
      "\t13.57s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 3.98709\n",
      "[2000]\tvalid_set's rmse: 3.92522\n",
      "[3000]\tvalid_set's rmse: 3.9037\n",
      "[4000]\tvalid_set's rmse: 3.88413\n",
      "[5000]\tvalid_set's rmse: 3.87263\n",
      "[6000]\tvalid_set's rmse: 3.86414\n",
      "[7000]\tvalid_set's rmse: 3.85463\n",
      "[8000]\tvalid_set's rmse: 3.85004\n",
      "[9000]\tvalid_set's rmse: 3.84715\n",
      "[10000]\tvalid_set's rmse: 3.84339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.8434\t = Validation score   (-root_mean_squared_error)\n",
      "\t2751.42s\t = Training   runtime\n",
      "\t8.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-3.7168\t = Validation score   (-root_mean_squared_error)\n",
      "\t9028.68s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-3.8859\t = Validation score   (-root_mean_squared_error)\n",
      "\t7173.86s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-8.3763\t = Validation score   (-root_mean_squared_error)\n",
      "\t1727.4s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    }
   ],
   "source": [
    "#sampled_line = 100000\n",
    "with Timer(f\"Load train full\"):\n",
    "    train_data = pd.read_csv(\"nyc_taxi_fare_train.csv\", usecols=cols)\n",
    "\n",
    "with Timer(\"Data Wrangling for train\"):\n",
    "    train_data = clean_df(train_data)\n",
    "\n",
    "model_for_full_data = TabularPredictor(label=\"fare_amount\")\n",
    "predictor_from_full = model_for_full_data.fit(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
