{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Taxi Fare Prediction with PyTorch\n",
    "\n",
    "### *read data (csv)* -> *data preprocess* -> *train model* -> *predict using the trained model*\n",
    "### pandas -> pandas.abs() -> Torch (SGD) -> ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run it on mutlti-nodes, need to run below shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,2]<stdout>:Data Load Consuming Time:\n",
      "[1,2]<stdout>:0:01:20.078336\n",
      "[1,2]<stdout>:finished\n",
      "[1,2]<stdout>:key                    0\n",
      "[1,2]<stdout>:fare_amount            0\n",
      "[1,2]<stdout>:pickup_datetime        0\n",
      "[1,2]<stdout>:pickup_longitude       0\n",
      "[1,2]<stdout>:pickup_latitude        0\n",
      "[1,2]<stdout>:dropoff_longitude    376\n",
      "[1,2]<stdout>:dropoff_latitude     376\n",
      "[1,2]<stdout>:passenger_count        0\n",
      "[1,2]<stdout>:dtype: int64\n",
      "[1,2]<stdout>:Old size 55423856\n",
      "[1,3]<stdout>:Data Load Consuming Time:\n",
      "[1,3]<stdout>:0:01:25.886857\n",
      "[1,3]<stdout>:finished\n",
      "[1,3]<stdout>:key                    0\n",
      "[1,3]<stdout>:fare_amount            0\n",
      "[1,3]<stdout>:pickup_datetime        0\n",
      "[1,3]<stdout>:pickup_longitude       0\n",
      "[1,3]<stdout>:pickup_latitude        0\n",
      "[1,3]<stdout>:dropoff_longitude    376\n",
      "[1,3]<stdout>:dropoff_latitude     376\n",
      "[1,3]<stdout>:passenger_count        0\n",
      "[1,3]<stdout>:dtype: int64\n",
      "[1,3]<stdout>:Old size 55423856\n",
      "[1,2]<stdout>:New size 55423480\n",
      "[1,3]<stdout>:New size 55423480\n",
      "[1,2]<stdout>:55308916\n",
      "[1,2]<stdout>:Data Processing Consuming Time:\n",
      "[1,2]<stdout>:0:00:25.039880\n",
      "[1,2]<stdout>:Data prepare Consuming Time:\n",
      "[1,2]<stdout>:0:01:45.118216\n",
      "[1,3]<stdout>:55308916\n",
      "[1,3]<stdout>:Data Processing Consuming Time:\n",
      "[1,3]<stdout>:0:00:30.023713\n",
      "[1,3]<stdout>:Data prepare Consuming Time:\n",
      "[1,3]<stdout>:0:01:55.910570\n",
      "[1,0]<stdout>:Data Load Consuming Time:\n",
      "[1,0]<stdout>:0:02:20.564105\n",
      "[1,0]<stdout>:finished\n",
      "[1,1]<stdout>:Data Load Consuming Time:\n",
      "[1,1]<stdout>:0:02:25.554606\n",
      "[1,1]<stdout>:finished\n",
      "[1,0]<stdout>:key                    0\n",
      "[1,0]<stdout>:fare_amount            0\n",
      "[1,0]<stdout>:pickup_datetime        0\n",
      "[1,0]<stdout>:pickup_longitude       0\n",
      "[1,0]<stdout>:pickup_latitude        0\n",
      "[1,0]<stdout>:dropoff_longitude    376\n",
      "[1,0]<stdout>:dropoff_latitude     376\n",
      "[1,0]<stdout>:passenger_count        0\n",
      "[1,0]<stdout>:dtype: int64\n",
      "[1,0]<stdout>:Old size 55423856\n",
      "[1,1]<stdout>:key                    0\n",
      "[1,1]<stdout>:fare_amount            0\n",
      "[1,1]<stdout>:pickup_datetime        0\n",
      "[1,1]<stdout>:pickup_longitude       0\n",
      "[1,1]<stdout>:pickup_latitude        0\n",
      "[1,1]<stdout>:dropoff_longitude    376\n",
      "[1,1]<stdout>:dropoff_latitude     376\n",
      "[1,1]<stdout>:passenger_count        0\n",
      "[1,1]<stdout>:dtype: int64\n",
      "[1,1]<stdout>:Old size 55423856\n",
      "[1,0]<stdout>:New size 55423480\n",
      "[1,1]<stdout>:New size 55423480\n",
      "[1,0]<stderr>:[2020-03-24 14:58:28.241729: W horovod/common/stall_inspector.cc:105] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. \n",
      "[1,0]<stderr>:Stalled ranks:\n",
      "[1,0]<stderr>:0: [broadcast.0.bias, broadcast.0.weight, broadcast.1.bias, broadcast.1.weight, broadcast.2.bias, broadcast.2.weight ...]\n",
      "[1,0]<stderr>:1: [broadcast.0.bias, broadcast.0.weight, broadcast.1.bias, broadcast.1.weight, broadcast.2.bias, broadcast.2.weight ...]\n",
      "[1,0]<stdout>:55308916\n",
      "[1,0]<stdout>:Data Processing Consuming Time:\n",
      "[1,0]<stdout>:0:01:37.149333\n",
      "[1,0]<stdout>:Data prepare Consuming Time:\n",
      "[1,0]<stdout>:0:03:57.713438\n",
      "[1,0]<stderr>:[2020-03-24 14:59:28.246090: W horovod/common/stall_inspector.cc:105] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. \n",
      "[1,0]<stderr>:Stalled ranks:\n",
      "[1,0]<stderr>:1: [broadcast.0.bias, broadcast.0.weight, broadcast.1.bias, broadcast.1.weight, broadcast.2.bias, broadcast.2.weight ...]\n",
      "[1,1]<stdout>:55308916\n",
      "[1,1]<stdout>:Data Processing Consuming Time:\n",
      "[1,1]<stdout>:0:01:48.599249\n",
      "[1,1]<stdout>:Data prepare Consuming Time:\n",
      "[1,1]<stdout>:0:04:14.153855\n",
      "[1,2]<stdout>:Train Size:\n",
      "[1,2]<stdout>:38716241\n",
      "[1,3]<stdout>:Train Size:\n",
      "[1,3]<stdout>:38716241\n",
      "[1,1]<stdout>:Train Size:\n",
      "[1,1]<stdout>:38716241\n",
      "[1,0]<stdout>:Train Size:\n",
      "[1,0]<stdout>:38716241\n",
      "[1,2]<stdout>:epoch:  0  loss:  323.5205078125\n",
      "[1,3]<stdout>:epoch:  0  loss:  323.5205078125\n",
      "[1,0]<stdout>:epoch:  0  loss:  323.5205078125\n",
      "[1,1]<stdout>:epoch:  0  loss:  323.5205078125\n",
      "[1,2]<stdout>:epoch:  1  loss:  311.805908203125\n",
      "[1,0]<stdout>:epoch:  1  loss:  311.805908203125\n",
      "[1,1]<stdout>:epoch:  1  loss:  311.805908203125\n",
      "[1,3]<stdout>:epoch:  1  loss:  311.805908203125\n",
      "[1,1]<stdout>:epoch:  2  loss:  300.023193359375\n",
      "[1,3]<stdout>:epoch:  2  loss:  300.023193359375\n",
      "[1,0]<stdout>:epoch:  2  loss:  300.023193359375\n",
      "[1,2]<stdout>:epoch:  2  loss:  300.023193359375\n",
      "[1,1]<stdout>:epoch:  3  loss:  285.4942321777344\n",
      "[1,0]<stdout>:epoch:  3  loss:  285.4942321777344\n",
      "[1,3]<stdout>:epoch:  3  loss:  285.4942321777344\n",
      "[1,2]<stdout>:epoch:  3  loss:  285.4942321777344\n",
      "[1,0]<stdout>:epoch:  4  loss:  265.795166015625\n",
      "[1,1]<stdout>:epoch:  4  loss:  265.795166015625\n",
      "[1,3]<stdout>:epoch:  4  loss:  265.795166015625\n",
      "[1,2]<stdout>:epoch:  4  loss:  265.795166015625\n",
      "[1,1]<stdout>:epoch:  5  loss:  239.4132537841797\n",
      "[1,0]<stdout>:epoch:  5  loss:  239.4132537841797\n",
      "[1,2]<stdout>:epoch:  5  loss:  239.4132537841797\n",
      "[1,3]<stdout>:epoch:  5  loss:  239.4132537841797\n",
      "[1,3]<stdout>:epoch:  6  loss:  210.37049865722656\n",
      "[1,2]<stdout>:epoch:  6  loss:  210.37049865722656\n",
      "[1,0]<stdout>:epoch:  6  loss:  210.37049865722656\n",
      "[1,1]<stdout>:epoch:  6  loss:  210.37049865722656\n",
      "[1,3]<stdout>:epoch:  7  loss:  193.37820434570312\n",
      "[1,1]<stdout>:epoch:  7  loss:  193.37820434570312\n",
      "[1,2]<stdout>:epoch:  7  loss:  193.37820434570312\n",
      "[1,0]<stdout>:epoch:  7  loss:  193.37820434570312\n",
      "[1,1]<stdout>:epoch:  8  loss:  192.4212646484375\n",
      "[1,0]<stdout>:epoch:  8  loss:  192.4212646484375\n",
      "[1,3]<stdout>:epoch:  8  loss:  192.4212646484375\n",
      "[1,2]<stdout>:epoch:  8  loss:  192.4212646484375\n",
      "[1,2]<stdout>:epoch:  9  loss:  192.31808471679688\n",
      "[1,3]<stdout>:epoch:  9  loss:  192.31808471679688\n",
      "[1,1]<stdout>:epoch:  9  loss:  192.31808471679688\n",
      "[1,0]<stdout>:epoch:  9  loss:  192.31808471679688\n",
      "[1,3]<stdout>:epoch:  10  loss:  192.23606872558594\n",
      "[1,2]<stdout>:epoch:  10  loss:  192.23606872558594\n",
      "[1,0]<stdout>:epoch:  10  loss:  192.23606872558594\n",
      "[1,1]<stdout>:epoch:  10  loss:  192.23606872558594\n",
      "[1,0]<stdout>:epoch:  11  loss:  192.15545654296875\n",
      "[1,3]<stdout>:epoch:  11  loss:  192.15545654296875\n",
      "[1,2]<stdout>:epoch:  11  loss:  192.15545654296875\n",
      "[1,1]<stdout>:epoch:  11  loss:  192.15545654296875\n",
      "[1,0]<stdout>:epoch:  12  loss:  192.074462890625\n",
      "[1,2]<stdout>:epoch:  12  loss:  192.074462890625\n",
      "[1,3]<stdout>:epoch:  12  loss:  192.074462890625\n",
      "[1,1]<stdout>:epoch:  12  loss:  192.074462890625\n",
      "[1,3]<stdout>:epoch:  13  loss:  191.9929656982422\n",
      "[1,2]<stdout>:epoch:  13  loss:  191.9929656982422\n",
      "[1,0]<stdout>:epoch:  13  loss:  191.9929656982422\n",
      "[1,1]<stdout>:epoch:  13  loss:  191.9929656982422\n",
      "[1,0]<stdout>:epoch:  14  loss:  191.910888671875\n",
      "[1,3]<stdout>:epoch:  14  loss:  191.910888671875\n",
      "[1,2]<stdout>:epoch:  14  loss:  191.910888671875\n",
      "[1,1]<stdout>:epoch:  14  loss:  191.910888671875\n",
      "[1,0]<stdout>:epoch:  15  loss:  191.82815551757812\n",
      "[1,3]<stdout>:epoch:  15  loss:  191.82815551757812\n",
      "[1,2]<stdout>:epoch:  15  loss:  191.82815551757812\n",
      "[1,1]<stdout>:epoch:  15  loss:  191.82815551757812\n",
      "[1,0]<stdout>:epoch:  16  loss:  191.7447509765625\n",
      "[1,3]<stdout>:epoch:  16  loss:  191.7447509765625\n",
      "[1,2]<stdout>:epoch:  16  loss:  191.7447509765625\n",
      "[1,1]<stdout>:epoch:  16  loss:  191.7447509765625\n",
      "[1,0]<stdout>:epoch:  17  loss:  191.66058349609375\n",
      "[1,3]<stdout>:epoch:  17  loss:  191.66058349609375\n",
      "[1,1]<stdout>:epoch:  17  loss:  191.66058349609375\n",
      "[1,2]<stdout>:epoch:  17  loss:  191.66058349609375\n",
      "[1,0]<stdout>:epoch:  18  loss:  191.57562255859375\n",
      "[1,3]<stdout>:epoch:  18  loss:  191.57562255859375\n",
      "[1,1]<stdout>:epoch:  18  loss:  191.57562255859375\n",
      "[1,2]<stdout>:epoch:  18  loss:  191.57562255859375\n",
      "[1,0]<stdout>:epoch:  19  loss:  191.48980712890625\n",
      "[1,2]<stdout>:epoch:  19  loss:  191.48980712890625\n",
      "[1,3]<stdout>:epoch:  19  loss:  191.48980712890625\n",
      "[1,1]<stdout>:epoch:  19  loss:  191.48980712890625\n",
      "[1,0]<stdout>:epoch:  20  loss:  191.403076171875\n",
      "[1,3]<stdout>:epoch:  20  loss:  191.403076171875\n",
      "[1,1]<stdout>:epoch:  20  loss:  191.403076171875\n",
      "[1,2]<stdout>:epoch:  20  loss:  191.403076171875\n",
      "[1,0]<stdout>:epoch:  21  loss:  191.31539916992188\n",
      "[1,3]<stdout>:epoch:  21  loss:  191.31539916992188\n",
      "[1,1]<stdout>:epoch:  21  loss:  191.31539916992188\n",
      "[1,2]<stdout>:epoch:  21  loss:  191.31539916992188\n",
      "[1,0]<stdout>:epoch:  22  loss:  191.2266845703125\n",
      "[1,1]<stdout>:epoch:  22  loss:  191.2266845703125\n",
      "[1,3]<stdout>:epoch:  22  loss:  191.2266845703125\n",
      "[1,2]<stdout>:epoch:  22  loss:  191.2266845703125\n",
      "[1,1]<stdout>:epoch:  23  loss:  191.13682556152344\n",
      "[1,0]<stdout>:epoch:  23  loss:  191.13682556152344\n",
      "[1,3]<stdout>:epoch:  23  loss:  191.13682556152344\n",
      "[1,2]<stdout>:epoch:  23  loss:  191.13682556152344\n",
      "[1,0]<stdout>:epoch:  24  loss:  191.0457763671875\n",
      "[1,3]<stdout>:epoch:  24  loss:  191.0457763671875\n",
      "[1,1]<stdout>:epoch:  24  loss:  191.0457763671875\n",
      "[1,2]<stdout>:epoch:  24  loss:  191.0457763671875\n",
      "[1,0]<stdout>:epoch:  25  loss:  190.95346069335938\n",
      "[1,3]<stdout>:epoch:  25  loss:  190.95346069335938\n",
      "[1,2]<stdout>:epoch:  25  loss:  190.95346069335938\n",
      "[1,1]<stdout>:epoch:  25  loss:  190.95346069335938\n",
      "[1,0]<stdout>:epoch:  26  loss:  190.8598175048828\n",
      "[1,3]<stdout>:epoch:  26  loss:  190.8598175048828\n",
      "[1,2]<stdout>:epoch:  26  loss:  190.8598175048828\n",
      "[1,1]<stdout>:epoch:  26  loss:  190.8598175048828\n",
      "[1,0]<stdout>:epoch:  27  loss:  190.76480102539062\n",
      "[1,1]<stdout>:epoch:  27  loss:  190.76480102539062\n",
      "[1,3]<stdout>:epoch:  27  loss:  190.76480102539062\n",
      "[1,2]<stdout>:epoch:  27  loss:  190.76480102539062\n",
      "[1,2]<stdout>:epoch:  28  loss:  190.6683349609375\n",
      "[1,1]<stdout>:epoch:  28  loss:  190.6683349609375\n",
      "[1,0]<stdout>:epoch:  28  loss:  190.6683349609375\n",
      "[1,3]<stdout>:epoch:  28  loss:  190.6683349609375\n",
      "[1,0]<stdout>:epoch:  29  loss:  190.5703582763672\n",
      "[1,2]<stdout>:epoch:  29  loss:  190.5703582763672\n",
      "[1,3]<stdout>:epoch:  29  loss:  190.5703582763672\n",
      "[1,1]<stdout>:epoch:  29  loss:  190.5703582763672\n",
      "[1,0]<stdout>:epoch:  30  loss:  190.4707794189453\n",
      "[1,2]<stdout>:epoch:  30  loss:  190.4707794189453\n",
      "[1,3]<stdout>:epoch:  30  loss:  190.4707794189453\n",
      "[1,1]<stdout>:epoch:  30  loss:  190.4707794189453\n",
      "[1,0]<stdout>:epoch:  31  loss:  190.36956787109375\n",
      "[1,3]<stdout>:epoch:  31  loss:  190.36956787109375\n",
      "[1,2]<stdout>:epoch:  31  loss:  190.36956787109375\n",
      "[1,1]<stdout>:epoch:  31  loss:  190.36956787109375\n",
      "[1,0]<stdout>:epoch:  32  loss:  190.26657104492188\n",
      "[1,2]<stdout>:epoch:  32  loss:  190.26657104492188\n",
      "[1,3]<stdout>:epoch:  32  loss:  190.26657104492188\n",
      "[1,1]<stdout>:epoch:  32  loss:  190.26657104492188\n",
      "[1,0]<stdout>:epoch:  33  loss:  190.16168212890625\n",
      "[1,3]<stdout>:epoch:  33  loss:  190.16168212890625\n",
      "[1,2]<stdout>:epoch:  33  loss:  190.16168212890625\n",
      "[1,1]<stdout>:epoch:  33  loss:  190.16168212890625\n",
      "[1,1]<stdout>:epoch:  34  loss:  190.0548858642578\n",
      "[1,2]<stdout>:epoch:  34  loss:  190.0548858642578\n",
      "[1,3]<stdout>:epoch:  34  loss:  190.0548858642578\n",
      "[1,0]<stdout>:epoch:  34  loss:  190.0548858642578\n",
      "[1,1]<stdout>:epoch:  35  loss:  189.94593811035156\n",
      "[1,2]<stdout>:epoch:  35  loss:  189.94593811035156\n",
      "[1,0]<stdout>:epoch:  35  loss:  189.94593811035156\n",
      "[1,3]<stdout>:epoch:  35  loss:  189.94593811035156\n",
      "[1,1]<stdout>:epoch:  36  loss:  189.83474731445312\n",
      "[1,0]<stdout>:epoch:  36  loss:  189.83474731445312\n",
      "[1,3]<stdout>:epoch:  36  loss:  189.83474731445312\n",
      "[1,2]<stdout>:epoch:  36  loss:  189.83474731445312\n",
      "[1,2]<stdout>:epoch:  37  loss:  189.72128295898438\n",
      "[1,3]<stdout>:epoch:  37  loss:  189.72128295898438\n",
      "[1,0]<stdout>:epoch:  37  loss:  189.72128295898438\n",
      "[1,1]<stdout>:epoch:  37  loss:  189.72128295898438\n",
      "[1,0]<stdout>:epoch:  38  loss:  189.60537719726562\n",
      "[1,3]<stdout>:epoch:  38  loss:  189.60537719726562\n",
      "[1,1]<stdout>:epoch:  38  loss:  189.60537719726562\n",
      "[1,2]<stdout>:epoch:  38  loss:  189.60537719726562\n",
      "[1,3]<stdout>:epoch:  39  loss:  189.48687744140625\n",
      "[1,1]<stdout>:epoch:  39  loss:  189.48687744140625\n",
      "[1,0]<stdout>:epoch:  39  loss:  189.48687744140625\n",
      "[1,2]<stdout>:epoch:  39  loss:  189.48687744140625\n",
      "[1,0]<stdout>:epoch:  40  loss:  189.36561584472656\n",
      "[1,3]<stdout>:epoch:  40  loss:  189.36561584472656\n",
      "[1,1]<stdout>:epoch:  40  loss:  189.36561584472656\n",
      "[1,2]<stdout>:epoch:  40  loss:  189.36561584472656\n",
      "[1,0]<stdout>:epoch:  41  loss:  189.24156188964844\n",
      "[1,3]<stdout>:epoch:  41  loss:  189.24156188964844\n",
      "[1,2]<stdout>:epoch:  41  loss:  189.24156188964844\n",
      "[1,1]<stdout>:epoch:  41  loss:  189.24156188964844\n",
      "[1,1]<stdout>:epoch:  42  loss:  189.11456298828125\n",
      "[1,3]<stdout>:epoch:  42  loss:  189.11456298828125\n",
      "[1,0]<stdout>:epoch:  42  loss:  189.11456298828125\n",
      "[1,2]<stdout>:epoch:  42  loss:  189.11456298828125\n",
      "[1,3]<stdout>:epoch:  43  loss:  188.9844207763672\n",
      "[1,1]<stdout>:epoch:  43  loss:  188.9844207763672\n",
      "[1,0]<stdout>:epoch:  43  loss:  188.9844207763672\n",
      "[1,2]<stdout>:epoch:  43  loss:  188.9844207763672\n",
      "[1,1]<stdout>:epoch:  44  loss:  188.8507843017578\n",
      "[1,0]<stdout>:epoch:  44  loss:  188.8507843017578\n",
      "[1,3]<stdout>:epoch:  44  loss:  188.8507843017578\n",
      "[1,2]<stdout>:epoch:  44  loss:  188.8507843017578\n",
      "[1,2]<stdout>:epoch:  45  loss:  188.7135009765625\n",
      "[1,0]<stdout>:epoch:  45  loss:  188.7135009765625\n",
      "[1,1]<stdout>:epoch:  45  loss:  188.7135009765625\n",
      "[1,3]<stdout>:epoch:  45  loss:  188.7135009765625\n",
      "[1,0]<stdout>:epoch:  46  loss:  188.57252502441406\n",
      "[1,1]<stdout>:epoch:  46  loss:  188.57252502441406\n",
      "[1,3]<stdout>:epoch:  46  loss:  188.57252502441406\n",
      "[1,2]<stdout>:epoch:  46  loss:  188.57252502441406\n",
      "[1,0]<stdout>:epoch:  47  loss:  188.42776489257812\n",
      "[1,3]<stdout>:epoch:  47  loss:  188.42776489257812\n",
      "[1,2]<stdout>:epoch:  47  loss:  188.42776489257812\n",
      "[1,1]<stdout>:epoch:  47  loss:  188.42776489257812\n",
      "[1,0]<stdout>:epoch:  48  loss:  188.2789306640625\n",
      "[1,3]<stdout>:epoch:  48  loss:  188.2789306640625\n",
      "[1,2]<stdout>:epoch:  48  loss:  188.2789306640625\n",
      "[1,1]<stdout>:epoch:  48  loss:  188.2789306640625\n",
      "[1,0]<stdout>:epoch:  49  loss:  188.125732421875\n",
      "[1,2]<stdout>:epoch:  49  loss:  188.125732421875\n",
      "[1,3]<stdout>:epoch:  49  loss:  188.125732421875\n",
      "[1,1]<stdout>:epoch:  49  loss:  188.125732421875\n",
      "[1,2]<stdout>:epoch:  50  loss:  187.96786499023438\n",
      "[1,3]<stdout>:epoch:  50  loss:  187.96786499023438\n",
      "[1,0]<stdout>:epoch:  50  loss:  187.96786499023438\n",
      "[1,1]<stdout>:epoch:  50  loss:  187.96786499023438\n",
      "[1,0]<stdout>:epoch:  51  loss:  187.80517578125\n",
      "[1,1]<stdout>:epoch:  51  loss:  187.80517578125\n",
      "[1,2]<stdout>:epoch:  51  loss:  187.80517578125\n",
      "[1,3]<stdout>:epoch:  51  loss:  187.80517578125\n",
      "[1,0]<stdout>:epoch:  52  loss:  187.6373291015625\n",
      "[1,2]<stdout>:epoch:  52  loss:  187.6373291015625\n",
      "[1,3]<stdout>:epoch:  52  loss:  187.6373291015625\n",
      "[1,1]<stdout>:epoch:  52  loss:  187.6373291015625\n",
      "[1,0]<stdout>:epoch:  53  loss:  187.4639892578125\n",
      "[1,3]<stdout>:epoch:  53  loss:  187.4639892578125\n",
      "[1,2]<stdout>:epoch:  53  loss:  187.4639892578125\n",
      "[1,1]<stdout>:epoch:  53  loss:  187.4639892578125\n",
      "[1,1]<stdout>:epoch:  54  loss:  187.28512573242188\n",
      "[1,0]<stdout>:epoch:  54  loss:  187.28512573242188\n",
      "[1,3]<stdout>:epoch:  54  loss:  187.28512573242188\n",
      "[1,2]<stdout>:epoch:  54  loss:  187.28512573242188\n",
      "[1,0]<stdout>:epoch:  55  loss:  187.1005401611328\n",
      "[1,3]<stdout>:epoch:  55  loss:  187.1005401611328\n",
      "[1,2]<stdout>:epoch:  55  loss:  187.1005401611328\n",
      "[1,1]<stdout>:epoch:  55  loss:  187.1005401611328\n",
      "[1,0]<stdout>:epoch:  56  loss:  186.90977478027344\n",
      "[1,3]<stdout>:epoch:  56  loss:  186.90977478027344\n",
      "[1,1]<stdout>:epoch:  56  loss:  186.90977478027344\n",
      "[1,2]<stdout>:epoch:  56  loss:  186.90977478027344\n",
      "[1,3]<stdout>:epoch:  57  loss:  186.71238708496094\n",
      "[1,2]<stdout>:epoch:  57  loss:  186.71238708496094\n",
      "[1,0]<stdout>:epoch:  57  loss:  186.71238708496094\n",
      "[1,1]<stdout>:epoch:  57  loss:  186.71238708496094\n",
      "[1,3]<stdout>:epoch:  58  loss:  186.50796508789062\n",
      "[1,2]<stdout>:epoch:  58  loss:  186.50796508789062\n",
      "[1,0]<stdout>:epoch:  58  loss:  186.50796508789062\n",
      "[1,1]<stdout>:epoch:  58  loss:  186.50796508789062\n",
      "[1,1]<stdout>:epoch:  59  loss:  186.29629516601562\n",
      "[1,3]<stdout>:epoch:  59  loss:  186.29629516601562\n",
      "[1,2]<stdout>:epoch:  59  loss:  186.29629516601562\n",
      "[1,0]<stdout>:epoch:  59  loss:  186.29629516601562\n",
      "[1,3]<stdout>:epoch:  60  loss:  186.07681274414062\n",
      "[1,1]<stdout>:epoch:  60  loss:  186.07681274414062\n",
      "[1,0]<stdout>:epoch:  60  loss:  186.07681274414062\n",
      "[1,2]<stdout>:epoch:  60  loss:  186.07681274414062\n",
      "[1,0]<stdout>:epoch:  61  loss:  185.84852600097656\n",
      "[1,1]<stdout>:epoch:  61  loss:  185.84852600097656\n",
      "[1,3]<stdout>:epoch:  61  loss:  185.84852600097656\n",
      "[1,2]<stdout>:epoch:  61  loss:  185.84852600097656\n",
      "[1,1]<stdout>:epoch:  62  loss:  185.61119079589844\n",
      "[1,2]<stdout>:epoch:  62  loss:  185.61119079589844\n",
      "[1,3]<stdout>:epoch:  62  loss:  185.61119079589844\n",
      "[1,0]<stdout>:epoch:  62  loss:  185.61119079589844\n",
      "[1,1]<stdout>:epoch:  63  loss:  185.3646697998047\n",
      "[1,3]<stdout>:epoch:  63  loss:  185.3646697998047\n",
      "[1,2]<stdout>:epoch:  63  loss:  185.3646697998047\n",
      "[1,0]<stdout>:epoch:  63  loss:  185.3646697998047\n",
      "[1,0]<stdout>:epoch:  64  loss:  185.1084747314453\n",
      "[1,3]<stdout>:epoch:  64  loss:  185.1084747314453\n",
      "[1,2]<stdout>:epoch:  64  loss:  185.1084747314453\n",
      "[1,1]<stdout>:epoch:  64  loss:  185.1084747314453\n",
      "[1,1]<stdout>:epoch:  65  loss:  184.84190368652344\n",
      "[1,3]<stdout>:epoch:  65  loss:  184.84190368652344\n",
      "[1,0]<stdout>:epoch:  65  loss:  184.84190368652344\n",
      "[1,2]<stdout>:epoch:  65  loss:  184.84190368652344\n",
      "[1,1]<stdout>:epoch:  66  loss:  184.56419372558594\n",
      "[1,3]<stdout>:epoch:  66  loss:  184.56419372558594\n",
      "[1,2]<stdout>:epoch:  66  loss:  184.56419372558594\n",
      "[1,0]<stdout>:epoch:  66  loss:  184.56419372558594\n",
      "[1,3]<stdout>:epoch:  67  loss:  184.27513122558594\n",
      "[1,0]<stdout>:epoch:  67  loss:  184.27513122558594\n",
      "[1,1]<stdout>:epoch:  67  loss:  184.27513122558594\n",
      "[1,2]<stdout>:epoch:  67  loss:  184.27513122558594\n",
      "[1,0]<stdout>:epoch:  68  loss:  183.97451782226562\n",
      "[1,3]<stdout>:epoch:  68  loss:  183.97451782226562\n",
      "[1,2]<stdout>:epoch:  68  loss:  183.97451782226562\n",
      "[1,1]<stdout>:epoch:  68  loss:  183.97451782226562\n",
      "[1,0]<stdout>:epoch:  69  loss:  183.6598358154297\n",
      "[1,2]<stdout>:epoch:  69  loss:  183.6598358154297\n",
      "[1,1]<stdout>:epoch:  69  loss:  183.6598358154297\n",
      "[1,3]<stdout>:epoch:  69  loss:  183.6598358154297\n",
      "[1,0]<stdout>:epoch:  70  loss:  183.33140563964844\n",
      "[1,3]<stdout>:epoch:  70  loss:  183.33140563964844\n",
      "[1,2]<stdout>:epoch:  70  loss:  183.33140563964844\n",
      "[1,1]<stdout>:epoch:  70  loss:  183.33140563964844\n",
      "[1,0]<stdout>:epoch:  71  loss:  182.99002075195312\n",
      "[1,2]<stdout>:epoch:  71  loss:  182.99002075195312\n",
      "[1,1]<stdout>:epoch:  71  loss:  182.99002075195312\n",
      "[1,3]<stdout>:epoch:  71  loss:  182.99002075195312\n",
      "[1,1]<stdout>:epoch:  72  loss:  182.63307189941406\n",
      "[1,3]<stdout>:epoch:  72  loss:  182.63307189941406\n",
      "[1,0]<stdout>:epoch:  72  loss:  182.63307189941406\n",
      "[1,2]<stdout>:epoch:  72  loss:  182.63307189941406\n",
      "[1,3]<stdout>:epoch:  73  loss:  182.2593994140625\n",
      "[1,0]<stdout>:epoch:  73  loss:  182.2593994140625\n",
      "[1,2]<stdout>:epoch:  73  loss:  182.2593994140625\n",
      "[1,1]<stdout>:epoch:  73  loss:  182.2593994140625\n",
      "[1,2]<stdout>:epoch:  74  loss:  181.86888122558594\n",
      "[1,0]<stdout>:epoch:  74  loss:  181.86888122558594\n",
      "[1,3]<stdout>:epoch:  74  loss:  181.86888122558594\n",
      "[1,1]<stdout>:epoch:  74  loss:  181.86888122558594\n",
      "[1,3]<stdout>:epoch:  75  loss:  181.46139526367188\n",
      "[1,1]<stdout>:epoch:  75  loss:  181.46139526367188\n",
      "[1,2]<stdout>:epoch:  75  loss:  181.46139526367188\n",
      "[1,0]<stdout>:epoch:  75  loss:  181.46139526367188\n",
      "[1,3]<stdout>:epoch:  76  loss:  181.03533935546875\n",
      "[1,2]<stdout>:epoch:  76  loss:  181.03533935546875\n",
      "[1,0]<stdout>:epoch:  76  loss:  181.03533935546875\n",
      "[1,1]<stdout>:epoch:  76  loss:  181.03533935546875\n",
      "[1,2]<stdout>:epoch:  77  loss:  180.58998107910156\n",
      "[1,3]<stdout>:epoch:  77  loss:  180.58998107910156\n",
      "[1,0]<stdout>:epoch:  77  loss:  180.58998107910156\n",
      "[1,1]<stdout>:epoch:  77  loss:  180.58998107910156\n",
      "[1,3]<stdout>:epoch:  78  loss:  180.1241455078125\n",
      "[1,2]<stdout>:epoch:  78  loss:  180.1241455078125\n",
      "[1,1]<stdout>:epoch:  78  loss:  180.1241455078125\n",
      "[1,0]<stdout>:epoch:  78  loss:  180.1241455078125\n",
      "[1,2]<stdout>:epoch:  79  loss:  179.63719177246094\n",
      "[1,0]<stdout>:epoch:  79  loss:  179.63719177246094\n",
      "[1,1]<stdout>:epoch:  79  loss:  179.63719177246094\n",
      "[1,3]<stdout>:epoch:  79  loss:  179.63719177246094\n",
      "[1,0]<stdout>:epoch:  80  loss:  179.1280517578125\n",
      "[1,1]<stdout>:epoch:  80  loss:  179.1280517578125\n",
      "[1,2]<stdout>:epoch:  80  loss:  179.1280517578125\n",
      "[1,3]<stdout>:epoch:  80  loss:  179.1280517578125\n",
      "[1,2]<stdout>:epoch:  81  loss:  178.59600830078125\n",
      "[1,3]<stdout>:epoch:  81  loss:  178.59600830078125\n",
      "[1,1]<stdout>:epoch:  81  loss:  178.59600830078125\n",
      "[1,0]<stdout>:epoch:  81  loss:  178.59600830078125\n",
      "[1,2]<stdout>:epoch:  82  loss:  178.0400848388672\n",
      "[1,1]<stdout>:epoch:  82  loss:  178.0400848388672\n",
      "[1,0]<stdout>:epoch:  82  loss:  178.0400848388672\n",
      "[1,3]<stdout>:epoch:  82  loss:  178.0400848388672\n",
      "[1,3]<stdout>:epoch:  83  loss:  177.45944213867188\n",
      "[1,0]<stdout>:epoch:  83  loss:  177.45944213867188\n",
      "[1,1]<stdout>:epoch:  83  loss:  177.45944213867188\n",
      "[1,2]<stdout>:epoch:  83  loss:  177.45944213867188\n",
      "[1,2]<stdout>:epoch:  84  loss:  176.8534698486328\n",
      "[1,3]<stdout>:epoch:  84  loss:  176.8534698486328\n",
      "[1,0]<stdout>:epoch:  84  loss:  176.8534698486328\n",
      "[1,1]<stdout>:epoch:  84  loss:  176.8534698486328\n",
      "[1,3]<stdout>:epoch:  85  loss:  176.2209014892578\n",
      "[1,1]<stdout>:epoch:  85  loss:  176.2209014892578\n",
      "[1,0]<stdout>:epoch:  85  loss:  176.2209014892578\n",
      "[1,2]<stdout>:epoch:  85  loss:  176.2209014892578\n",
      "[1,3]<stdout>:epoch:  86  loss:  175.56190490722656\n",
      "[1,0]<stdout>:epoch:  86  loss:  175.56190490722656\n",
      "[1,1]<stdout>:epoch:  86  loss:  175.56190490722656\n",
      "[1,2]<stdout>:epoch:  86  loss:  175.56190490722656\n",
      "[1,3]<stdout>:epoch:  87  loss:  174.87562561035156\n",
      "[1,2]<stdout>:epoch:  87  loss:  174.87562561035156\n",
      "[1,0]<stdout>:epoch:  87  loss:  174.87562561035156\n",
      "[1,1]<stdout>:epoch:  87  loss:  174.87562561035156\n",
      "[1,0]<stdout>:epoch:  88  loss:  174.16294860839844\n",
      "[1,3]<stdout>:epoch:  88  loss:  174.16294860839844\n",
      "[1,2]<stdout>:epoch:  88  loss:  174.16294860839844\n",
      "[1,1]<stdout>:epoch:  88  loss:  174.16294860839844\n",
      "[1,0]<stdout>:epoch:  89  loss:  173.427001953125\n",
      "[1,1]<stdout>:epoch:  89  loss:  173.427001953125\n",
      "[1,3]<stdout>:epoch:  89  loss:  173.427001953125\n",
      "[1,2]<stdout>:epoch:  89  loss:  173.427001953125\n",
      "[1,2]<stdout>:epoch:  90  loss:  172.69296264648438\n",
      "[1,0]<stdout>:epoch:  90  loss:  172.69296264648438\n",
      "[1,3]<stdout>:epoch:  90  loss:  172.69296264648438\n",
      "[1,1]<stdout>:epoch:  90  loss:  172.69296264648438\n",
      "[1,2]<stdout>:epoch:  91  loss:  172.0977325439453\n",
      "[1,0]<stdout>:epoch:  91  loss:  172.0977325439453\n",
      "[1,1]<stdout>:epoch:  91  loss:  172.0977325439453\n",
      "[1,3]<stdout>:epoch:  91  loss:  172.0977325439453\n",
      "[1,0]<stdout>:epoch:  92  loss:  172.66893005371094\n",
      "[1,1]<stdout>:epoch:  92  loss:  172.66893005371094\n",
      "[1,2]<stdout>:epoch:  92  loss:  172.66893005371094\n",
      "[1,3]<stdout>:epoch:  92  loss:  172.66893005371094\n",
      "[1,0]<stdout>:epoch:  93  loss:  182.25328063964844\n",
      "[1,2]<stdout>:epoch:  93  loss:  182.25328063964844\n",
      "[1,1]<stdout>:epoch:  93  loss:  182.25328063964844\n",
      "[1,3]<stdout>:epoch:  93  loss:  182.25328063964844\n",
      "[1,2]<stdout>:epoch:  94  loss:  269.4482116699219\n",
      "[1,0]<stdout>:epoch:  94  loss:  269.4482116699219\n",
      "[1,1]<stdout>:epoch:  94  loss:  269.4482116699219\n",
      "[1,3]<stdout>:epoch:  94  loss:  269.4482116699219\n",
      "[1,3]<stdout>:epoch:  95  loss:  401.0867614746094\n",
      "[1,2]<stdout>:epoch:  95  loss:  401.0867614746094\n",
      "[1,1]<stdout>:epoch:  95  loss:  401.0867614746094\n",
      "[1,0]<stdout>:epoch:  95  loss:  401.0867614746094\n",
      "[1,0]<stdout>:epoch:  96  loss:  372.47991943359375\n",
      "[1,1]<stdout>:epoch:  96  loss:  372.47991943359375\n",
      "[1,2]<stdout>:epoch:  96  loss:  372.47991943359375\n",
      "[1,3]<stdout>:epoch:  96  loss:  372.47991943359375\n",
      "[1,3]<stdout>:epoch:  97  loss:  300.4758605957031\n",
      "[1,1]<stdout>:epoch:  97  loss:  300.4758605957031\n",
      "[1,2]<stdout>:epoch:  97  loss:  300.4758605957031\n",
      "[1,0]<stdout>:epoch:  97  loss:  300.4758605957031\n",
      "[1,3]<stdout>:epoch:  98  loss:  282.0951232910156\n",
      "[1,1]<stdout>:epoch:  98  loss:  282.0951232910156\n",
      "[1,0]<stdout>:epoch:  98  loss:  282.0951232910156\n",
      "[1,2]<stdout>:epoch:  98  loss:  282.0951232910156\n",
      "[1,0]<stdout>:epoch:  99  loss:  267.2374572753906\n",
      "[1,2]<stdout>:epoch:  99  loss:  267.2374572753906\n",
      "[1,1]<stdout>:epoch:  99  loss:  267.2374572753906\n",
      "[1,3]<stdout>:epoch:  99  loss:  267.2374572753906\n",
      "[1,2]<stdout>:epoch:  0  loss:  251.54127502441406\n",
      "[1,0]<stdout>:epoch:  100  loss:  251.54127502441406\n",
      "[1,1]<stdout>:epoch:  100  loss:  251.54127502441406\n",
      "[1,3]<stdout>:epoch:  0  loss:  251.54127502441406\n",
      "[1,3]<stdout>:epoch:  1  loss:  233.615966796875\n",
      "[1,2]<stdout>:epoch:  1  loss:  233.615966796875\n",
      "[1,0]<stdout>:epoch:  101  loss:  233.615966796875\n",
      "[1,1]<stdout>:epoch:  101  loss:  233.615966796875\n",
      "[1,3]<stdout>:epoch:  2  loss:  213.68667602539062\n",
      "[1,2]<stdout>:epoch:  2  loss:  213.68667602539062\n",
      "[1,0]<stdout>:epoch:  102  loss:  213.68667602539062\n",
      "[1,1]<stdout>:epoch:  102  loss:  213.68667602539062\n",
      "[1,3]<stdout>:epoch:  3  loss:  195.5016326904297\n",
      "[1,2]<stdout>:epoch:  3  loss:  195.5016326904297\n",
      "[1,0]<stdout>:epoch:  103  loss:  195.5016326904297\n",
      "[1,1]<stdout>:epoch:  103  loss:  195.5016326904297\n",
      "[1,3]<stdout>:epoch:  4  loss:  186.56851196289062\n",
      "[1,2]<stdout>:epoch:  4  loss:  186.56851196289062\n",
      "[1,1]<stdout>:epoch:  104  loss:  186.56851196289062\n",
      "[1,0]<stdout>:epoch:  104  loss:  186.56851196289062\n",
      "[1,3]<stdout>:epoch:  5  loss:  185.39205932617188\n",
      "[1,2]<stdout>:epoch:  5  loss:  185.39205932617188\n",
      "[1,1]<stdout>:epoch:  105  loss:  185.39205932617188\n",
      "[1,0]<stdout>:epoch:  105  loss:  185.39205932617188\n",
      "[1,3]<stdout>:epoch:  6  loss:  185.06570434570312\n",
      "[1,2]<stdout>:epoch:  6  loss:  185.06570434570312\n",
      "[1,1]<stdout>:epoch:  106  loss:  185.06570434570312\n",
      "[1,0]<stdout>:epoch:  106  loss:  185.06570434570312\n",
      "[1,3]<stdout>:epoch:  7  loss:  184.7343292236328\n",
      "[1,2]<stdout>:epoch:  7  loss:  184.7343292236328\n",
      "[1,1]<stdout>:epoch:  107  loss:  184.7343292236328\n",
      "[1,0]<stdout>:epoch:  107  loss:  184.7343292236328\n",
      "[1,1]<stdout>:epoch:  108  loss:  184.39157104492188\n",
      "[1,3]<stdout>:epoch:  8  loss:  184.39157104492188\n",
      "[1,2]<stdout>:epoch:  8  loss:  184.39157104492188\n",
      "[1,0]<stdout>:epoch:  108  loss:  184.39157104492188\n",
      "[1,2]<stdout>:epoch:  9  loss:  184.0361328125\n",
      "[1,3]<stdout>:epoch:  9  loss:  184.0361328125\n",
      "[1,1]<stdout>:epoch:  109  loss:  184.0361328125\n",
      "[1,0]<stdout>:epoch:  109  loss:  184.0361328125\n",
      "[1,0]<stdout>:epoch:  110  loss:  183.6664276123047\n",
      "[1,3]<stdout>:epoch:  10  loss:  183.6664276123047\n",
      "[1,1]<stdout>:epoch:  110  loss:  183.6664276123047\n",
      "[1,2]<stdout>:epoch:  10  loss:  183.6664276123047\n",
      "[1,0]<stdout>:epoch:  111  loss:  183.28192138671875\n",
      "[1,2]<stdout>:epoch:  11  loss:  183.28192138671875\n",
      "[1,1]<stdout>:epoch:  111  loss:  183.28192138671875\n",
      "[1,3]<stdout>:epoch:  11  loss:  183.28192138671875\n",
      "[1,0]<stdout>:epoch:  112  loss:  182.88169860839844\n",
      "[1,1]<stdout>:epoch:  112  loss:  182.88169860839844\n",
      "[1,3]<stdout>:epoch:  12  loss:  182.88169860839844\n",
      "[1,2]<stdout>:epoch:  12  loss:  182.88169860839844\n",
      "[1,0]<stdout>:epoch:  113  loss:  182.46450805664062\n",
      "[1,1]<stdout>:epoch:  113  loss:  182.46450805664062\n",
      "[1,3]<stdout>:epoch:  13  loss:  182.46450805664062\n",
      "[1,2]<stdout>:epoch:  13  loss:  182.46450805664062\n",
      "[1,1]<stdout>:epoch:  114  loss:  182.0289764404297\n",
      "[1,3]<stdout>:epoch:  14  loss:  182.0289764404297\n",
      "[1,2]<stdout>:epoch:  14  loss:  182.0289764404297\n",
      "[1,0]<stdout>:epoch:  114  loss:  182.0289764404297\n",
      "[1,2]<stdout>:epoch:  15  loss:  181.57391357421875\n",
      "[1,1]<stdout>:epoch:  115  loss:  181.57391357421875\n",
      "[1,0]<stdout>:epoch:  115  loss:  181.57391357421875\n",
      "[1,3]<stdout>:epoch:  15  loss:  181.57391357421875\n",
      "[1,0]<stdout>:epoch:  116  loss:  181.0987091064453\n",
      "[1,1]<stdout>:epoch:  116  loss:  181.0987091064453\n",
      "[1,2]<stdout>:epoch:  16  loss:  181.0987091064453\n",
      "[1,3]<stdout>:epoch:  16  loss:  181.0987091064453\n",
      "[1,3]<stdout>:epoch:  17  loss:  180.60118103027344\n",
      "[1,1]<stdout>:epoch:  117  loss:  180.60118103027344\n",
      "[1,0]<stdout>:epoch:  117  loss:  180.60118103027344\n",
      "[1,2]<stdout>:epoch:  17  loss:  180.60118103027344\n",
      "[1,1]<stdout>:epoch:  118  loss:  180.0807342529297\n",
      "[1,2]<stdout>:epoch:  18  loss:  180.0807342529297\n",
      "[1,3]<stdout>:epoch:  18  loss:  180.0807342529297\n",
      "[1,0]<stdout>:epoch:  118  loss:  180.0807342529297\n",
      "[1,1]<stdout>:epoch:  119  loss:  179.5364227294922\n",
      "[1,0]<stdout>:epoch:  119  loss:  179.5364227294922\n",
      "[1,3]<stdout>:epoch:  19  loss:  179.5364227294922\n",
      "[1,2]<stdout>:epoch:  19  loss:  179.5364227294922\n",
      "[1,3]<stdout>:epoch:  20  loss:  178.96621704101562\n",
      "[1,0]<stdout>:epoch:  120  loss:  178.96621704101562\n",
      "[1,1]<stdout>:epoch:  120  loss:  178.96621704101562\n",
      "[1,2]<stdout>:epoch:  20  loss:  178.96621704101562\n",
      "[1,0]<stdout>:epoch:  121  loss:  178.36915588378906\n",
      "[1,1]<stdout>:epoch:  121  loss:  178.36915588378906\n",
      "[1,3]<stdout>:epoch:  21  loss:  178.36915588378906\n",
      "[1,2]<stdout>:epoch:  21  loss:  178.36915588378906\n",
      "[1,3]<stdout>:epoch:  22  loss:  177.7443084716797\n",
      "[1,0]<stdout>:epoch:  122  loss:  177.7443084716797\n",
      "[1,1]<stdout>:epoch:  122  loss:  177.7443084716797\n",
      "[1,2]<stdout>:epoch:  22  loss:  177.7443084716797\n",
      "[1,1]<stdout>:epoch:  123  loss:  177.09083557128906\n",
      "[1,3]<stdout>:epoch:  23  loss:  177.09083557128906\n",
      "[1,0]<stdout>:epoch:  123  loss:  177.09083557128906\n",
      "[1,2]<stdout>:epoch:  23  loss:  177.09083557128906\n",
      "[1,3]<stdout>:epoch:  24  loss:  176.40744018554688\n",
      "[1,0]<stdout>:epoch:  124  loss:  176.40744018554688\n",
      "[1,2]<stdout>:epoch:  24  loss:  176.40744018554688\n",
      "[1,1]<stdout>:epoch:  124  loss:  176.40744018554688\n",
      "[1,3]<stdout>:epoch:  25  loss:  175.69406127929688\n",
      "[1,0]<stdout>:epoch:  125  loss:  175.69406127929688\n",
      "[1,1]<stdout>:epoch:  125  loss:  175.69406127929688\n",
      "[1,2]<stdout>:epoch:  25  loss:  175.69406127929688\n",
      "[1,2]<stdout>:epoch:  26  loss:  174.95025634765625\n",
      "[1,0]<stdout>:epoch:  126  loss:  174.95025634765625\n",
      "[1,3]<stdout>:epoch:  26  loss:  174.95025634765625\n",
      "[1,1]<stdout>:epoch:  126  loss:  174.95025634765625\n",
      "[1,3]<stdout>:epoch:  27  loss:  174.1757354736328\n",
      "[1,2]<stdout>:epoch:  27  loss:  174.1757354736328\n",
      "[1,1]<stdout>:epoch:  127  loss:  174.1757354736328\n",
      "[1,0]<stdout>:epoch:  127  loss:  174.1757354736328\n",
      "[1,2]<stdout>:epoch:  28  loss:  173.36997985839844\n",
      "[1,3]<stdout>:epoch:  28  loss:  173.36997985839844\n",
      "[1,1]<stdout>:epoch:  128  loss:  173.36997985839844\n",
      "[1,0]<stdout>:epoch:  128  loss:  173.36997985839844\n",
      "[1,3]<stdout>:epoch:  29  loss:  172.53334045410156\n",
      "[1,2]<stdout>:epoch:  29  loss:  172.53334045410156\n",
      "[1,0]<stdout>:epoch:  129  loss:  172.53334045410156\n",
      "[1,1]<stdout>:epoch:  129  loss:  172.53334045410156\n",
      "[1,3]<stdout>:epoch:  30  loss:  171.66607666015625\n",
      "[1,2]<stdout>:epoch:  30  loss:  171.66607666015625\n",
      "[1,1]<stdout>:epoch:  130  loss:  171.66607666015625\n",
      "[1,0]<stdout>:epoch:  130  loss:  171.66607666015625\n",
      "[1,1]<stdout>:epoch:  131  loss:  170.76853942871094\n",
      "[1,2]<stdout>:epoch:  31  loss:  170.76853942871094\n",
      "[1,3]<stdout>:epoch:  31  loss:  170.76853942871094\n",
      "[1,0]<stdout>:epoch:  131  loss:  170.76853942871094\n",
      "[1,3]<stdout>:epoch:  32  loss:  169.84205627441406\n",
      "[1,0]<stdout>:epoch:  132  loss:  169.84205627441406\n",
      "[1,2]<stdout>:epoch:  32  loss:  169.84205627441406\n",
      "[1,1]<stdout>:epoch:  132  loss:  169.84205627441406\n",
      "[1,2]<stdout>:epoch:  33  loss:  168.8874969482422\n",
      "[1,3]<stdout>:epoch:  33  loss:  168.8874969482422\n",
      "[1,0]<stdout>:epoch:  133  loss:  168.8874969482422\n",
      "[1,1]<stdout>:epoch:  133  loss:  168.8874969482422\n",
      "[1,3]<stdout>:epoch:  34  loss:  167.9069061279297\n",
      "[1,0]<stdout>:epoch:  134  loss:  167.9069061279297\n",
      "[1,1]<stdout>:epoch:  134  loss:  167.9069061279297\n",
      "[1,2]<stdout>:epoch:  34  loss:  167.9069061279297\n",
      "[1,3]<stdout>:epoch:  35  loss:  166.9024658203125\n",
      "[1,2]<stdout>:epoch:  35  loss:  166.9024658203125\n",
      "[1,0]<stdout>:epoch:  135  loss:  166.9024658203125\n",
      "[1,1]<stdout>:epoch:  135  loss:  166.9024658203125\n",
      "[1,3]<stdout>:epoch:  36  loss:  165.87689208984375\n",
      "[1,1]<stdout>:epoch:  136  loss:  165.87689208984375\n",
      "[1,0]<stdout>:epoch:  136  loss:  165.87689208984375\n",
      "[1,2]<stdout>:epoch:  36  loss:  165.87689208984375\n",
      "[1,2]<stdout>:epoch:  37  loss:  164.83392333984375\n",
      "[1,3]<stdout>:epoch:  37  loss:  164.83392333984375\n",
      "[1,0]<stdout>:epoch:  137  loss:  164.83392333984375\n",
      "[1,1]<stdout>:epoch:  137  loss:  164.83392333984375\n",
      "[1,0]<stdout>:epoch:  138  loss:  163.78636169433594\n",
      "[1,2]<stdout>:epoch:  38  loss:  163.78636169433594\n",
      "[1,1]<stdout>:epoch:  138  loss:  163.78636169433594\n",
      "[1,3]<stdout>:epoch:  38  loss:  163.78636169433594\n",
      "[1,2]<stdout>:epoch:  39  loss:  162.7856903076172\n",
      "[1,1]<stdout>:epoch:  139  loss:  162.7856903076172\n",
      "[1,0]<stdout>:epoch:  139  loss:  162.7856903076172\n",
      "[1,3]<stdout>:epoch:  39  loss:  162.7856903076172\n",
      "[1,2]<stdout>:epoch:  40  loss:  162.36891174316406\n",
      "[1,3]<stdout>:epoch:  40  loss:  162.36891174316406\n",
      "[1,0]<stdout>:epoch:  140  loss:  162.36891174316406\n",
      "[1,1]<stdout>:epoch:  140  loss:  162.36891174316406\n",
      "[1,2]<stdout>:epoch:  41  loss:  167.87484741210938\n",
      "[1,3]<stdout>:epoch:  41  loss:  167.87484741210938\n",
      "[1,1]<stdout>:epoch:  141  loss:  167.87484741210938\n",
      "[1,0]<stdout>:epoch:  141  loss:  167.87484741210938\n",
      "[1,3]<stdout>:epoch:  42  loss:  241.91233825683594\n",
      "[1,1]<stdout>:epoch:  142  loss:  241.91233825683594\n",
      "[1,2]<stdout>:epoch:  42  loss:  241.91233825683594\n",
      "[1,0]<stdout>:epoch:  142  loss:  241.91233825683594\n",
      "[1,3]<stdout>:epoch:  43  loss:  551.5919189453125\n",
      "[1,1]<stdout>:epoch:  143  loss:  551.5919189453125\n",
      "[1,0]<stdout>:epoch:  143  loss:  551.5919189453125\n",
      "[1,2]<stdout>:epoch:  43  loss:  551.5919189453125\n",
      "[1,3]<stdout>:epoch:  44  loss:  351.888671875\n",
      "[1,1]<stdout>:epoch:  144  loss:  351.888671875\n",
      "[1,2]<stdout>:epoch:  44  loss:  351.888671875\n",
      "[1,0]<stdout>:epoch:  144  loss:  351.888671875\n",
      "[1,3]<stdout>:epoch:  45  loss:  249.46046447753906\n",
      "[1,2]<stdout>:epoch:  45  loss:  249.46046447753906\n",
      "[1,1]<stdout>:epoch:  145  loss:  249.46046447753906\n",
      "[1,0]<stdout>:epoch:  145  loss:  249.46046447753906\n",
      "[1,2]<stdout>:epoch:  46  loss:  223.19866943359375\n",
      "[1,3]<stdout>:epoch:  46  loss:  223.19866943359375\n",
      "[1,0]<stdout>:epoch:  146  loss:  223.19866943359375\n",
      "[1,1]<stdout>:epoch:  146  loss:  223.19866943359375\n",
      "[1,3]<stdout>:epoch:  47  loss:  204.00880432128906\n",
      "[1,2]<stdout>:epoch:  47  loss:  204.00880432128906\n",
      "[1,0]<stdout>:epoch:  147  loss:  204.00880432128906\n",
      "[1,1]<stdout>:epoch:  147  loss:  204.00880432128906\n",
      "[1,3]<stdout>:epoch:  48  loss:  198.07647705078125\n",
      "[1,1]<stdout>:epoch:  148  loss:  198.07647705078125\n",
      "[1,0]<stdout>:epoch:  148  loss:  198.07647705078125\n",
      "[1,2]<stdout>:epoch:  48  loss:  198.07647705078125\n",
      "[1,3]<stdout>:epoch:  49  loss:  197.67636108398438\n",
      "[1,2]<stdout>:epoch:  49  loss:  197.67636108398438\n",
      "[1,0]<stdout>:epoch:  149  loss:  197.67636108398438\n",
      "[1,1]<stdout>:epoch:  149  loss:  197.67636108398438\n",
      "[1,1]<stdout>:epoch:  150  loss:  197.42340087890625\n",
      "[1,2]<stdout>:epoch:  50  loss:  197.42340087890625\n",
      "[1,3]<stdout>:epoch:  50  loss:  197.42340087890625\n",
      "[1,0]<stdout>:epoch:  150  loss:  197.42340087890625\n",
      "[1,3]<stdout>:epoch:  51  loss:  197.16873168945312\n",
      "[1,0]<stdout>:epoch:  151  loss:  197.16873168945312\n",
      "[1,1]<stdout>:epoch:  151  loss:  197.16873168945312\n"
     ]
    }
   ],
   "source": [
    "! horovodrun -np 4 -H hdfs1:2,hdfs2:2 python /home/sparkuser/jupyter/Bin/NYC_Taxi_Fare/pytorch_horovod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Add horovod with torch import\n",
    "import horovod.torch as hvd\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data by pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Consuming Time:\n",
      "0:01:17.617604\n",
      "finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "key                   object\n",
       "fare_amount          float64\n",
       "pickup_datetime       object\n",
       "pickup_longitude     float64\n",
       "pickup_latitude      float64\n",
       "dropoff_longitude    float64\n",
       "dropoff_latitude     float64\n",
       "passenger_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time1=datetime.now()\n",
    "PATH = './input'\n",
    "os.listdir(PATH)\n",
    "train_df = pd.read_csv(f'{PATH}/train.csv')\n",
    "#data size: 5.4GB \n",
    "time2=datetime.now()\n",
    "data_load_time=time2-time1\n",
    "print(\"Data Load Consuming Time:\")\n",
    "print(data_load_time)\n",
    "print(\"finished\")\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                    0\n",
      "fare_amount            0\n",
      "pickup_datetime        0\n",
      "pickup_longitude       0\n",
      "pickup_latitude        0\n",
      "dropoff_longitude    376\n",
      "dropoff_latitude     376\n",
      "passenger_count        0\n",
      "dtype: int64\n",
      "Old size 55423856\n",
      "New size 55423480\n",
      "55308916\n",
      "Data Processing Consuming Time:\n",
      "0:00:24.247500\n",
      "Data prepare Consuming Time:\n",
      "0:01:41.865104\n"
     ]
    }
   ],
   "source": [
    "time3=datetime.now()\n",
    "# Check NaNs in the dataset\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "\n",
    "print('Old size %d'% len(train_df))\n",
    "train_df = train_df.dropna(how='any',axis='rows')\n",
    "print('New size %d' % len(train_df))\n",
    "# travel vectors between start and end points for the taxi ride, in both longitude and latitude coordinates \n",
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "\n",
    "# remove the bizzare travelling distance\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "add_travel_vector_features(train_df)\n",
    "\n",
    "#plot a subset of travel vector to see its distribution \n",
    "#plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')\n",
    "\n",
    "#there are some further data processing skiped below\n",
    "\n",
    "#We expect most of these values to be very small (likely between 0 and 1) since it should all \n",
    "#be differences between GPS coordinates within one city. For reference, one degree of latitude is about 69 miles. \n",
    "#However, we can see the dataset has extreme values which do not make sense. \n",
    "#Let's remove those values from our training set. Based on the scatterplot, \n",
    "#it looks like we can safely exclude values above 5 (though remember the scatterplot is only showing the first 2000 rows...)\n",
    "\n",
    "#print('Old size: %d' % len(train_df))\n",
    "#train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "#print('New size: %d' % len(train_df))\n",
    "\n",
    "\n",
    "train_df = train_df[(train_df.abs_diff_longitude<5) & (train_df.abs_diff_latitude<5)]\n",
    "print(len(train_df))\n",
    "\n",
    "time4=datetime.now()\n",
    "data_processing_time=time4-time3\n",
    "print(\"Data Processing Consuming Time:\")\n",
    "print(data_processing_time)\n",
    "data_prepare_time=data_load_time+data_processing_time\n",
    "print(\"Data prepare Consuming Time:\")\n",
    "print(data_prepare_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training using Horovod with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:\n",
      "38716241\n",
      "epoch:  0  loss:  309.7013244628906\n",
      "epoch:  1  loss:  296.62841796875\n",
      "epoch:  2  loss:  281.2621765136719\n",
      "epoch:  3  loss:  260.4698181152344\n",
      "epoch:  4  loss:  233.14085388183594\n",
      "epoch:  5  loss:  205.31649780273438\n",
      "epoch:  6  loss:  192.37327575683594\n",
      "epoch:  7  loss:  191.79986572265625\n",
      "epoch:  8  loss:  191.70750427246094\n",
      "epoch:  9  loss:  191.6248321533203\n",
      "epoch:  10  loss:  191.5419464111328\n",
      "epoch:  11  loss:  191.4582061767578\n",
      "epoch:  12  loss:  191.37356567382812\n",
      "epoch:  13  loss:  191.28790283203125\n",
      "epoch:  14  loss:  191.2012176513672\n",
      "epoch:  15  loss:  191.11337280273438\n",
      "epoch:  16  loss:  191.02439880371094\n",
      "epoch:  17  loss:  190.93421936035156\n",
      "epoch:  18  loss:  190.84268188476562\n",
      "epoch:  19  loss:  190.74977111816406\n",
      "epoch:  20  loss:  190.65550231933594\n",
      "epoch:  21  loss:  190.5596923828125\n",
      "epoch:  22  loss:  190.46214294433594\n",
      "epoch:  23  loss:  190.36280822753906\n",
      "epoch:  24  loss:  190.26174926757812\n",
      "epoch:  25  loss:  190.1588592529297\n",
      "epoch:  26  loss:  190.05398559570312\n",
      "epoch:  27  loss:  189.9469757080078\n",
      "epoch:  28  loss:  189.83775329589844\n",
      "epoch:  29  loss:  189.7261505126953\n",
      "epoch:  30  loss:  189.6120147705078\n",
      "epoch:  31  loss:  189.495361328125\n",
      "epoch:  32  loss:  189.37600708007812\n",
      "epoch:  33  loss:  189.25379943847656\n",
      "epoch:  34  loss:  189.12857055664062\n",
      "epoch:  35  loss:  189.00018310546875\n",
      "epoch:  36  loss:  188.8684844970703\n",
      "epoch:  37  loss:  188.73336791992188\n",
      "epoch:  38  loss:  188.59446716308594\n",
      "epoch:  39  loss:  188.4514923095703\n",
      "epoch:  40  loss:  188.30430603027344\n",
      "epoch:  41  loss:  188.15301513671875\n",
      "epoch:  42  loss:  187.99737548828125\n",
      "epoch:  43  loss:  187.83676147460938\n",
      "epoch:  44  loss:  187.67042541503906\n",
      "epoch:  45  loss:  187.4984893798828\n",
      "epoch:  46  loss:  187.3212890625\n",
      "epoch:  47  loss:  187.13821411132812\n",
      "epoch:  48  loss:  186.948486328125\n",
      "epoch:  49  loss:  186.75180053710938\n",
      "epoch:  50  loss:  186.5484619140625\n",
      "epoch:  51  loss:  186.33746337890625\n",
      "epoch:  52  loss:  186.11856079101562\n",
      "epoch:  53  loss:  185.89108276367188\n",
      "epoch:  54  loss:  185.65476989746094\n",
      "epoch:  55  loss:  185.40907287597656\n",
      "epoch:  56  loss:  185.1535186767578\n",
      "epoch:  57  loss:  184.88739013671875\n",
      "epoch:  58  loss:  184.61032104492188\n",
      "epoch:  59  loss:  184.3219451904297\n",
      "epoch:  60  loss:  184.0211944580078\n",
      "epoch:  61  loss:  183.70713806152344\n",
      "epoch:  62  loss:  183.37930297851562\n",
      "epoch:  63  loss:  183.0373992919922\n",
      "epoch:  64  loss:  182.68023681640625\n",
      "epoch:  65  loss:  182.30673217773438\n",
      "epoch:  66  loss:  181.916259765625\n",
      "epoch:  67  loss:  181.50823974609375\n",
      "epoch:  68  loss:  181.0819854736328\n",
      "epoch:  69  loss:  180.63609313964844\n",
      "epoch:  70  loss:  180.16966247558594\n",
      "epoch:  71  loss:  179.6815185546875\n",
      "epoch:  72  loss:  179.17166137695312\n",
      "epoch:  73  loss:  178.63870239257812\n",
      "epoch:  74  loss:  178.08160400390625\n",
      "epoch:  75  loss:  177.49990844726562\n",
      "epoch:  76  loss:  176.89263916015625\n",
      "epoch:  77  loss:  176.25921630859375\n",
      "epoch:  78  loss:  175.59896850585938\n",
      "epoch:  79  loss:  174.9132080078125\n",
      "epoch:  80  loss:  174.20387268066406\n",
      "epoch:  81  loss:  173.49526977539062\n",
      "epoch:  82  loss:  172.89222717285156\n",
      "epoch:  83  loss:  173.11367797851562\n",
      "epoch:  84  loss:  178.8003387451172\n",
      "epoch:  85  loss:  229.0220184326172\n",
      "epoch:  86  loss:  358.7384948730469\n",
      "epoch:  87  loss:  403.9451599121094\n",
      "epoch:  88  loss:  291.5448303222656\n",
      "epoch:  89  loss:  275.5408020019531\n",
      "epoch:  90  loss:  259.477294921875\n",
      "epoch:  91  loss:  240.1903076171875\n",
      "epoch:  92  loss:  217.4114227294922\n",
      "epoch:  93  loss:  195.71534729003906\n",
      "epoch:  94  loss:  184.97402954101562\n",
      "epoch:  95  loss:  183.7753448486328\n",
      "epoch:  96  loss:  183.47337341308594\n",
      "epoch:  97  loss:  183.1576385498047\n",
      "epoch:  98  loss:  182.82737731933594\n",
      "epoch:  99  loss:  182.48162841796875\n",
      "epoch:  0  loss:  182.11915588378906\n",
      "epoch:  1  loss:  182.08154296875\n",
      "epoch:  2  loss:  182.0437774658203\n",
      "epoch:  3  loss:  182.0058135986328\n",
      "epoch:  4  loss:  181.96768188476562\n",
      "epoch:  5  loss:  181.9293670654297\n",
      "epoch:  6  loss:  181.89083862304688\n",
      "epoch:  7  loss:  181.85214233398438\n",
      "epoch:  8  loss:  181.813232421875\n",
      "epoch:  9  loss:  181.7741241455078\n",
      "epoch:  10  loss:  181.73484802246094\n",
      "epoch:  11  loss:  181.6953582763672\n",
      "epoch:  12  loss:  181.6556854248047\n",
      "epoch:  13  loss:  181.6157989501953\n",
      "epoch:  14  loss:  181.57571411132812\n",
      "epoch:  15  loss:  181.53543090820312\n",
      "epoch:  16  loss:  181.4949493408203\n",
      "epoch:  17  loss:  181.4542694091797\n",
      "epoch:  18  loss:  181.4133758544922\n",
      "epoch:  19  loss:  181.37228393554688\n",
      "epoch:  20  loss:  181.33099365234375\n",
      "epoch:  21  loss:  181.28948974609375\n",
      "epoch:  22  loss:  181.24777221679688\n",
      "epoch:  23  loss:  181.20584106445312\n",
      "epoch:  24  loss:  181.16372680664062\n",
      "epoch:  25  loss:  181.1213836669922\n",
      "epoch:  26  loss:  181.07884216308594\n",
      "epoch:  27  loss:  181.0360870361328\n",
      "epoch:  28  loss:  180.99314880371094\n",
      "epoch:  29  loss:  180.94998168945312\n",
      "epoch:  30  loss:  180.90660095214844\n",
      "epoch:  31  loss:  180.8629913330078\n",
      "epoch:  32  loss:  180.81918334960938\n",
      "epoch:  33  loss:  180.775146484375\n",
      "epoch:  34  loss:  180.73089599609375\n",
      "epoch:  35  loss:  180.6864013671875\n",
      "epoch:  36  loss:  180.64169311523438\n",
      "epoch:  37  loss:  180.5967559814453\n",
      "epoch:  38  loss:  180.55160522460938\n",
      "epoch:  39  loss:  180.5062255859375\n",
      "epoch:  40  loss:  180.46063232421875\n",
      "epoch:  41  loss:  180.41481018066406\n",
      "epoch:  42  loss:  180.36874389648438\n",
      "epoch:  43  loss:  180.32241821289062\n",
      "epoch:  44  loss:  180.27587890625\n",
      "epoch:  45  loss:  180.2290802001953\n",
      "epoch:  46  loss:  180.18202209472656\n",
      "epoch:  47  loss:  180.13473510742188\n",
      "epoch:  48  loss:  180.0872039794922\n",
      "epoch:  49  loss:  180.0394287109375\n",
      "epoch:  50  loss:  179.99139404296875\n",
      "epoch:  51  loss:  179.94313049316406\n",
      "epoch:  52  loss:  179.89462280273438\n",
      "epoch:  53  loss:  179.8458709716797\n",
      "epoch:  54  loss:  179.796875\n",
      "epoch:  55  loss:  179.74761962890625\n",
      "epoch:  56  loss:  179.6981201171875\n",
      "epoch:  57  loss:  179.6483917236328\n",
      "epoch:  58  loss:  179.598388671875\n",
      "epoch:  59  loss:  179.54815673828125\n",
      "epoch:  60  loss:  179.4976806640625\n",
      "epoch:  61  loss:  179.44696044921875\n",
      "epoch:  62  loss:  179.39598083496094\n",
      "epoch:  63  loss:  179.34474182128906\n",
      "epoch:  64  loss:  179.2932586669922\n",
      "epoch:  65  loss:  179.2415008544922\n",
      "epoch:  66  loss:  179.1894989013672\n",
      "epoch:  67  loss:  179.13726806640625\n",
      "epoch:  68  loss:  179.08474731445312\n",
      "epoch:  69  loss:  179.03199768066406\n",
      "epoch:  70  loss:  178.97898864746094\n",
      "epoch:  71  loss:  178.9257049560547\n",
      "epoch:  72  loss:  178.87216186523438\n",
      "epoch:  73  loss:  178.81834411621094\n",
      "epoch:  74  loss:  178.7642364501953\n",
      "epoch:  75  loss:  178.70986938476562\n",
      "epoch:  76  loss:  178.6552276611328\n",
      "epoch:  77  loss:  178.6002960205078\n",
      "epoch:  78  loss:  178.5450897216797\n",
      "epoch:  79  loss:  178.48959350585938\n",
      "epoch:  80  loss:  178.43382263183594\n",
      "epoch:  81  loss:  178.3777618408203\n",
      "epoch:  82  loss:  178.3214111328125\n",
      "epoch:  83  loss:  178.2647705078125\n",
      "epoch:  84  loss:  178.20782470703125\n",
      "epoch:  85  loss:  178.15060424804688\n",
      "epoch:  86  loss:  178.0930938720703\n",
      "epoch:  87  loss:  178.03529357910156\n",
      "epoch:  88  loss:  177.9772186279297\n",
      "epoch:  89  loss:  177.91883850097656\n",
      "epoch:  90  loss:  177.8601837158203\n",
      "epoch:  91  loss:  177.80120849609375\n",
      "epoch:  92  loss:  177.74197387695312\n",
      "epoch:  93  loss:  177.68243408203125\n",
      "epoch:  94  loss:  177.6226043701172\n",
      "epoch:  95  loss:  177.56248474121094\n",
      "epoch:  96  loss:  177.50205993652344\n",
      "epoch:  97  loss:  177.4413604736328\n",
      "epoch:  98  loss:  177.38034057617188\n",
      "epoch:  99  loss:  177.3190460205078\n",
      "epoch:  100  loss:  177.25746154785156\n",
      "epoch:  101  loss:  177.19557189941406\n",
      "epoch:  102  loss:  177.13339233398438\n",
      "epoch:  103  loss:  177.0709228515625\n",
      "epoch:  104  loss:  177.00816345214844\n",
      "epoch:  105  loss:  176.9451141357422\n",
      "epoch:  106  loss:  176.8817596435547\n",
      "epoch:  107  loss:  176.81813049316406\n",
      "epoch:  108  loss:  176.75418090820312\n",
      "epoch:  109  loss:  176.68992614746094\n",
      "epoch:  110  loss:  176.62538146972656\n",
      "epoch:  111  loss:  176.56051635742188\n",
      "epoch:  112  loss:  176.49533081054688\n",
      "epoch:  113  loss:  176.42982482910156\n",
      "epoch:  114  loss:  176.36398315429688\n",
      "epoch:  115  loss:  176.29783630371094\n",
      "epoch:  116  loss:  176.23135375976562\n",
      "epoch:  117  loss:  176.16456604003906\n",
      "epoch:  118  loss:  176.09742736816406\n",
      "epoch:  119  loss:  176.02999877929688\n",
      "epoch:  120  loss:  175.96221923828125\n",
      "epoch:  121  loss:  175.89414978027344\n",
      "epoch:  122  loss:  175.8257293701172\n",
      "epoch:  123  loss:  175.7570037841797\n",
      "epoch:  124  loss:  175.68795776367188\n",
      "epoch:  125  loss:  175.6186065673828\n",
      "epoch:  126  loss:  175.54893493652344\n",
      "epoch:  127  loss:  175.4789581298828\n",
      "epoch:  128  loss:  175.40866088867188\n",
      "epoch:  129  loss:  175.3380584716797\n",
      "epoch:  130  loss:  175.2671661376953\n",
      "epoch:  131  loss:  175.19593811035156\n",
      "epoch:  132  loss:  175.12440490722656\n",
      "epoch:  133  loss:  175.0525360107422\n",
      "epoch:  134  loss:  174.9803466796875\n",
      "epoch:  135  loss:  174.90782165527344\n",
      "epoch:  136  loss:  174.83499145507812\n",
      "epoch:  137  loss:  174.76181030273438\n",
      "epoch:  138  loss:  174.68832397460938\n",
      "epoch:  139  loss:  174.61447143554688\n",
      "epoch:  140  loss:  174.54031372070312\n",
      "epoch:  141  loss:  174.46578979492188\n",
      "epoch:  142  loss:  174.39097595214844\n",
      "epoch:  143  loss:  174.3157958984375\n",
      "epoch:  144  loss:  174.2403106689453\n",
      "epoch:  145  loss:  174.16448974609375\n",
      "epoch:  146  loss:  174.08834838867188\n",
      "epoch:  147  loss:  174.01187133789062\n",
      "epoch:  148  loss:  173.93507385253906\n",
      "epoch:  149  loss:  173.8579559326172\n",
      "epoch:  150  loss:  173.78053283691406\n",
      "epoch:  151  loss:  173.70277404785156\n",
      "epoch:  152  loss:  173.62469482421875\n",
      "epoch:  153  loss:  173.54629516601562\n",
      "epoch:  154  loss:  173.46759033203125\n",
      "epoch:  155  loss:  173.3885498046875\n",
      "epoch:  156  loss:  173.3092041015625\n",
      "epoch:  157  loss:  173.22952270507812\n",
      "epoch:  158  loss:  173.1495361328125\n",
      "epoch:  159  loss:  173.0692138671875\n",
      "epoch:  160  loss:  172.98858642578125\n",
      "epoch:  161  loss:  172.90762329101562\n",
      "epoch:  162  loss:  172.8263397216797\n",
      "epoch:  163  loss:  172.74473571777344\n",
      "epoch:  164  loss:  172.6627960205078\n",
      "epoch:  165  loss:  172.58053588867188\n",
      "epoch:  166  loss:  172.49795532226562\n",
      "epoch:  167  loss:  172.41502380371094\n",
      "epoch:  168  loss:  172.331787109375\n",
      "epoch:  169  loss:  172.2482147216797\n",
      "epoch:  170  loss:  172.16433715820312\n",
      "epoch:  171  loss:  172.0801239013672\n",
      "epoch:  172  loss:  171.99559020996094\n",
      "epoch:  173  loss:  171.91075134277344\n",
      "epoch:  174  loss:  171.82559204101562\n",
      "epoch:  175  loss:  171.7401123046875\n",
      "epoch:  176  loss:  171.65431213378906\n",
      "epoch:  177  loss:  171.56822204589844\n",
      "epoch:  178  loss:  171.4818115234375\n",
      "epoch:  179  loss:  171.3950958251953\n",
      "epoch:  180  loss:  171.30804443359375\n",
      "epoch:  181  loss:  171.22071838378906\n",
      "epoch:  182  loss:  171.133056640625\n",
      "epoch:  183  loss:  171.0450897216797\n",
      "epoch:  184  loss:  170.95681762695312\n",
      "epoch:  185  loss:  170.8682098388672\n",
      "epoch:  186  loss:  170.77932739257812\n",
      "epoch:  187  loss:  170.69012451171875\n",
      "epoch:  188  loss:  170.60061645507812\n",
      "epoch:  189  loss:  170.5108184814453\n",
      "epoch:  190  loss:  170.42071533203125\n",
      "epoch:  191  loss:  170.330322265625\n",
      "epoch:  192  loss:  170.2396240234375\n",
      "epoch:  193  loss:  170.14865112304688\n",
      "epoch:  194  loss:  170.05735778808594\n",
      "epoch:  195  loss:  169.96578979492188\n",
      "epoch:  196  loss:  169.87391662597656\n",
      "epoch:  197  loss:  169.78176879882812\n",
      "epoch:  198  loss:  169.6893310546875\n",
      "epoch:  199  loss:  169.5966033935547\n",
      "epoch:  200  loss:  169.50360107421875\n",
      "epoch:  201  loss:  169.41029357910156\n",
      "epoch:  202  loss:  169.31671142578125\n",
      "epoch:  203  loss:  169.2228546142578\n",
      "epoch:  204  loss:  169.12872314453125\n",
      "epoch:  205  loss:  169.0343017578125\n",
      "epoch:  206  loss:  168.93960571289062\n",
      "epoch:  207  loss:  168.8446502685547\n",
      "epoch:  208  loss:  168.74942016601562\n",
      "epoch:  209  loss:  168.6539306640625\n",
      "epoch:  210  loss:  168.55816650390625\n",
      "epoch:  211  loss:  168.46214294433594\n",
      "epoch:  212  loss:  168.36585998535156\n",
      "epoch:  213  loss:  168.26930236816406\n",
      "epoch:  214  loss:  168.17251586914062\n",
      "epoch:  215  loss:  168.07545471191406\n",
      "epoch:  216  loss:  167.9781494140625\n",
      "epoch:  217  loss:  167.880615234375\n",
      "epoch:  218  loss:  167.78280639648438\n",
      "epoch:  219  loss:  167.6847686767578\n",
      "epoch:  220  loss:  167.58648681640625\n",
      "epoch:  221  loss:  167.4879608154297\n",
      "epoch:  222  loss:  167.3892059326172\n",
      "epoch:  223  loss:  167.29022216796875\n",
      "epoch:  224  loss:  167.1909942626953\n",
      "epoch:  225  loss:  167.09153747558594\n",
      "epoch:  226  loss:  166.9918670654297\n",
      "epoch:  227  loss:  166.8919677734375\n",
      "epoch:  228  loss:  166.7918243408203\n",
      "epoch:  229  loss:  166.69149780273438\n",
      "epoch:  230  loss:  166.59092712402344\n",
      "epoch:  231  loss:  166.49017333984375\n",
      "epoch:  232  loss:  166.38922119140625\n",
      "epoch:  233  loss:  166.2880401611328\n",
      "epoch:  234  loss:  166.18667602539062\n",
      "epoch:  235  loss:  166.08511352539062\n",
      "epoch:  236  loss:  165.98336791992188\n",
      "epoch:  237  loss:  165.8814239501953\n",
      "epoch:  238  loss:  165.77928161621094\n",
      "epoch:  239  loss:  165.6769561767578\n",
      "epoch:  240  loss:  165.574462890625\n",
      "epoch:  241  loss:  165.47178649902344\n",
      "epoch:  242  loss:  165.36892700195312\n",
      "epoch:  243  loss:  165.26589965820312\n",
      "epoch:  244  loss:  165.1627197265625\n",
      "epoch:  245  loss:  165.05935668945312\n",
      "epoch:  246  loss:  164.95584106445312\n",
      "epoch:  247  loss:  164.85218811035156\n",
      "epoch:  248  loss:  164.74835205078125\n",
      "epoch:  249  loss:  164.64439392089844\n",
      "epoch:  250  loss:  164.54029846191406\n",
      "epoch:  251  loss:  164.43605041503906\n",
      "epoch:  252  loss:  164.3316650390625\n",
      "epoch:  253  loss:  164.22715759277344\n",
      "epoch:  254  loss:  164.1225128173828\n",
      "epoch:  255  loss:  164.01776123046875\n",
      "epoch:  256  loss:  163.9128875732422\n",
      "epoch:  257  loss:  163.80789184570312\n",
      "epoch:  258  loss:  163.70277404785156\n",
      "epoch:  259  loss:  163.59756469726562\n",
      "epoch:  260  loss:  163.4922637939453\n",
      "epoch:  261  loss:  163.3868408203125\n",
      "epoch:  262  loss:  163.28134155273438\n",
      "epoch:  263  loss:  163.17575073242188\n",
      "epoch:  264  loss:  163.07005310058594\n",
      "epoch:  265  loss:  162.96429443359375\n",
      "epoch:  266  loss:  162.85845947265625\n",
      "epoch:  267  loss:  162.75254821777344\n",
      "epoch:  268  loss:  162.64657592773438\n",
      "epoch:  269  loss:  162.54052734375\n",
      "epoch:  270  loss:  162.4344482421875\n",
      "epoch:  271  loss:  162.3282928466797\n",
      "epoch:  272  loss:  162.2220916748047\n",
      "epoch:  273  loss:  162.11585998535156\n",
      "epoch:  274  loss:  162.0095977783203\n",
      "epoch:  275  loss:  161.90328979492188\n",
      "epoch:  276  loss:  161.7969512939453\n",
      "epoch:  277  loss:  161.69058227539062\n",
      "epoch:  278  loss:  161.58419799804688\n",
      "epoch:  279  loss:  161.47781372070312\n",
      "epoch:  280  loss:  161.37139892578125\n",
      "epoch:  281  loss:  161.26498413085938\n",
      "epoch:  282  loss:  161.1585693359375\n",
      "epoch:  283  loss:  161.0521697998047\n",
      "epoch:  284  loss:  160.94578552246094\n",
      "epoch:  285  loss:  160.83941650390625\n",
      "epoch:  286  loss:  160.73304748535156\n",
      "epoch:  287  loss:  160.62672424316406\n",
      "epoch:  288  loss:  160.5204315185547\n",
      "epoch:  289  loss:  160.41416931152344\n",
      "epoch:  290  loss:  160.30795288085938\n",
      "epoch:  291  loss:  160.20176696777344\n",
      "epoch:  292  loss:  160.0956573486328\n",
      "epoch:  293  loss:  159.98959350585938\n",
      "epoch:  294  loss:  159.8835906982422\n",
      "epoch:  295  loss:  159.7776641845703\n",
      "epoch:  296  loss:  159.67181396484375\n",
      "epoch:  297  loss:  159.56602478027344\n",
      "epoch:  298  loss:  159.4603271484375\n",
      "epoch:  299  loss:  159.354736328125\n",
      "epoch:  300  loss:  159.24920654296875\n",
      "epoch:  301  loss:  159.14381408691406\n",
      "epoch:  302  loss:  159.0384979248047\n",
      "epoch:  303  loss:  158.9333038330078\n",
      "epoch:  304  loss:  158.82823181152344\n",
      "epoch:  305  loss:  158.7232666015625\n",
      "epoch:  306  loss:  158.61843872070312\n",
      "epoch:  307  loss:  158.51376342773438\n",
      "epoch:  308  loss:  158.40919494628906\n",
      "epoch:  309  loss:  158.30477905273438\n",
      "epoch:  310  loss:  158.2005157470703\n",
      "epoch:  311  loss:  158.0963897705078\n",
      "epoch:  312  loss:  157.99244689941406\n",
      "epoch:  313  loss:  157.88865661621094\n",
      "epoch:  314  loss:  157.7850341796875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "time5=datetime.now()\n",
    "\n",
    "#A sequential container. Modules will be added to it in the order they are passed in the constructor. \n",
    "#Alternatively, an ordered dict of modules can also be passed in.\n",
    "\n",
    "#一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，\n",
    "#同时以神经网络模块为元素的有序字典也可以作为传入参数。\n",
    "\n",
    "#a three layer NN model \n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10),\n",
    "                     nn.Linear(10, 5),\n",
    "                      nn.Linear(5, 1))\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# bigger learning rate \n",
    "optimizer1 = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "#Add Horovod Distributed Optimizer\n",
    "optimizer = hvd.DistributedOptimizer(optimizer1, named_parameters=model.named_parameters())\n",
    "# Broadcast parameters from rank 0 to all other processes.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "X = np.stack((train_df.abs_diff_latitude.values,train_df.abs_diff_longitude.values)).T\n",
    "X = torch.from_numpy(X)\n",
    "X = X.type(torch.FloatTensor)\n",
    "\n",
    "y = torch.from_numpy(train_df.fare_amount.values.T)\n",
    "y = y.type(torch.FloatTensor)\n",
    "y.unsqueeze_(-1)\n",
    "X_train, X_evalutation, y_train, y_evalutation = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "print(\"Train Size:\")\n",
    "print(len(X_train))\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(X_train)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # smaller LR \n",
    "optimizer2 = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Met some issue when using horovord, so we switched to single nodes \n",
    "# Next: Figure out the reason \n",
    "#optimizer = hvd.DistributedOptimizer(optimizer2, named_parameters=model.named_parameters())\n",
    "# Broadcast parameters from rank 0 to all other processes.\n",
    "#hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "for epoch in range(1500):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(X_train)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "time6=datetime.now()\n",
    "model_train_time=time6-time5\n",
    "print(\"Model Train Consuming Time:\")\n",
    "print(model_train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0238, 0.0207],\n",
      "        [0.0485, 0.0407],\n",
      "        [0.0046, 0.0031],\n",
      "        ...,\n",
      "        [0.0869, 0.0625],\n",
      "        [0.0261, 0.1157],\n",
      "        [0.0167, 0.0270]])\n",
      "tensor([[11.2868],\n",
      "        [16.1786],\n",
      "        [ 7.1883],\n",
      "        ...,\n",
      "        [22.4268],\n",
      "        [25.2755],\n",
      "        [11.6330]], grad_fn=<AddmmBackward>)\n",
      "RMSE Value:\n",
      "tensor(6.7698, grad_fn=<SqrtBackward>)\n",
      "Evalutation Consuming Time:\n",
      "0:00:00.066143\n",
      "Data Load Consuming Time:\n",
      "0:00:13.623180\n",
      "Data prepare Consuming Time:\n",
      "0:00:17.561898\n",
      "Model Train Consuming Time:\n",
      "0:23:56.400328\n",
      "Evalutation Consuming Time:\n",
      "0:00:00.066143\n",
      "Total Consuming Time:\n",
      "0:24:27.651549\n"
     ]
    }
   ],
   "source": [
    "time7=datetime.now()\n",
    "\n",
    "def RMSE(x,y):\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = torch.sqrt(criterion(x, y))\n",
    "    return loss\n",
    "print(X_evalutation)\n",
    "y_evalutation_result=model(X_evalutation)\n",
    "print (y_evalutation_result)\n",
    "\n",
    "rmse=RMSE(y_evalutation_result,y_evalutation)\n",
    "\n",
    "print(\"RMSE Value:\")\n",
    "print(rmse)\n",
    "\n",
    "time8=datetime.now()\n",
    "evalutation_time=time8-time7\n",
    "print(\"Evalutation Consuming Time:\")\n",
    "print(evalutation_time)\n",
    "\n",
    "print(\"Data Load Consuming Time:\")\n",
    "print(data_load_time)\n",
    "print(\"Data prepare Consuming Time:\")\n",
    "print(data_prepare_time)\n",
    "print(\"Model Train Consuming Time:\")\n",
    "print(model_train_time)\n",
    "print(\"Evalutation Consuming Time:\")\n",
    "print(evalutation_time)\n",
    "total_time=data_load_time+data_prepare_time+model_train_time+evalutation_time\n",
    "print(\"Total Consuming Time:\")\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.144782  9.701579  7.190537 ... 42.10261  18.43134   8.558159]\n",
      "['input', 'submission.csv', 'PyTorch_NY_Taxi_Fare_Predict_backup.ipynb', '.ipynb_checkpoints', 'PyTorch_NY_Taxi_Fare_Predict.ipynb']\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(f'{PATH}/test.csv')\n",
    "add_travel_vector_features(test_df)\n",
    "X_test = np.stack((test_df.abs_diff_latitude.values,test_df.abs_diff_longitude.values)).T\n",
    "X_test = torch.from_numpy(X_test)\n",
    "X_test = X_test.type(torch.FloatTensor)\n",
    "y_test = model(X_test)\n",
    "\n",
    "y_test = y_test.detach().numpy()\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {'key': test_df.key, 'fare_amount': y_test},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
