{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3243c04",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Geod\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75144be9",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7d5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutomizedCoordinationFix(df):\n",
    "    df = df.assign(rev=df.dropoff_latitude<df.dropoff_longitude)\n",
    "    idx = (df['rev'] == 1)\n",
    "    df.loc[idx,['dropoff_longitude','dropoff_latitude']] = df.loc[idx,['dropoff_latitude','dropoff_longitude']].values\n",
    "    df.loc[idx,['pickup_longitude','pickup_latitude']] = df.loc[idx,['pickup_latitude','pickup_longitude']].values\n",
    "    df = df.drop(columns=['rev'])\n",
    "    return df\n",
    "\n",
    "def clean_df(df):    \n",
    "    #reverse incorrectly assigned longitude/latitude values\n",
    "    df = cutomizedCoordinationFix(df)\n",
    "    df = df[(df.fare_amount > 0)  & (df.fare_amount <= 500) &\n",
    "          (df.passenger_count >= 0) & (df.passenger_count <= 8)  &\n",
    "           ((df.pickup_longitude != 0) & (df.pickup_latitude != 0) & (df.dropoff_longitude != 0) & (df.dropoff_latitude != 0) )]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc697874",
   "metadata": {},
   "source": [
    "# Use Featuretools to create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a917ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-02 20:40:06,048 featuretools - WARNING    Featuretools failed to load \"nlp_primitives\" primitives from \"nlp_primitives\". For a full stack trace, set logging to debug.\n",
      "2022-12-02 20:40:06,062 featuretools - WARNING    Featuretools failed to load plugin nlp_primitives from library nlp_primitives. For a full stack trace, set logging to debug.\n",
      "featuretools version is 1.18.0\n"
     ]
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "print(f\"featuretools version is {ft.__version__}\")\n",
    "\n",
    "from featuretools.primitives import TransformPrimitive\n",
    "from woodwork.column_schema import ColumnSchema\n",
    "from woodwork.logical_types import Double, LatLong, Datetime, Boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac4767",
   "metadata": {},
   "source": [
    "# featuretools related function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba178b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from woodwork.logical_types import Ordinal\n",
    "\n",
    "def produce_featuretools_entityset(es, df):\n",
    "    trip_logical_types = {\n",
    "        'passenger_count': Ordinal(order=list(range(0, 10))), \n",
    "        'pickup_latlong': 'LatLong',\n",
    "        'dropoff_latlong': 'LatLong',\n",
    "    }\n",
    "\n",
    "    es.add_dataframe(dataframe_name=\"trips\",\n",
    "                     dataframe=df,\n",
    "                     index=\"id\",\n",
    "                     time_index='pickup_datetime',\n",
    "                     logical_types=trip_logical_types)\n",
    "\n",
    "    return es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f87974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.primitives import IsInGeoBox\n",
    "\n",
    "class Bearing(TransformPrimitive):\n",
    "    name = \"bearing\"\n",
    "    input_types = [ColumnSchema(logical_type=LatLong), ColumnSchema(logical_type=LatLong)]\n",
    "    return_type = ColumnSchema(logical_type=Double, semantic_tags={'numeric'})\n",
    "    number_output_features = 1\n",
    "    commutative=True\n",
    "    def get_function(self):\n",
    "        def bearing(latlong1, latlong2):\n",
    "            lat1 = np.array([x[0] for x in latlong1])\n",
    "            lon1 = np.array([x[1] for x in latlong1])\n",
    "            lat2 = np.array([x[0] for x in latlong2])\n",
    "            lon2 = np.array([x[1] for x in latlong2])\n",
    "            delta_lon = np.radians(lon2 - lon1)\n",
    "            lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "            x = np.cos(lat2) * np.sin(delta_lon)\n",
    "            y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(delta_lon)\n",
    "            return np.degrees(np.arctan2(x, y))\n",
    "        return bearing\n",
    "    \n",
    "class DistanceToLocation(TransformPrimitive):\n",
    "    name = \"distance_to_location\"\n",
    "    input_types = [ColumnSchema(logical_type=LatLong)]\n",
    "    return_type = ColumnSchema(logical_type=Double, semantic_tags={'numeric'})\n",
    "    number_output_features = 1\n",
    "    commutative=True\n",
    "    def __init__(self, point=(0, 0)):\n",
    "        self.point = point\n",
    "        self.lat = point[0]\n",
    "        self.lon = point[1]\n",
    "        \n",
    "    def get_function(self):\n",
    "        def distance_to_location(latlong):\n",
    "            lat = np.array([x[0] for x in latlong])\n",
    "            lon = np.array([x[1] for x in latlong])\n",
    "            tgt_lat = len(lat) * self.lat\n",
    "            tgt_lon = len(lon) * self.lon\n",
    "            return self.sphere_dist(tgt_lat, tgt_lon, lat, lon)\n",
    "        return distance_to_location\n",
    "    \n",
    "    def sphere_dist(self, lat1, lon1, lat2, lon2):\n",
    "        \"\"\"\n",
    "        Return distance along great radius between pickup and dropoff coordinates.\n",
    "        \"\"\"\n",
    "        #Define earth radius (km)\n",
    "        R_earth = 6371\n",
    "        #Convert degrees to radians\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians,[lat1, lon1, lat2, lon2])\n",
    "        #Compute distances along lat, lon dimensions\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "\n",
    "        #Compute haversine distance\n",
    "        a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "        return 2 * R_earth * np.arcsin(np.sqrt(a))\n",
    "\n",
    "\n",
    "def get_coordination(df):\n",
    "    df[\"pickup_latlong\"] = df[['pickup_latitude', 'pickup_longitude']].apply(tuple, axis=1)\n",
    "    df[\"dropoff_latlong\"] = df[['dropoff_latitude', 'dropoff_longitude']].apply(tuple, axis=1)\n",
    "    df = df.drop([\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "def modelling_features(df, feature_list = None, features_only = False):\n",
    "    df = get_coordination(df)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    es = ft.EntitySet(\"nyc_taxi_fare\")\n",
    "    es = produce_featuretools_entityset(es, df)\n",
    "    \n",
    "    cutoff_time = es['trips'][['id', 'pickup_datetime']]\n",
    "    \n",
    "    if feature_list:\n",
    "        df = ft.calculate_feature_matrix(feature_list, entityset=es, cutoff_time=cutoff_time, verbose=True)\n",
    "        return df, es, feature_list\n",
    "    \n",
    "    # airport coordination\n",
    "    coordination_dicts = {\n",
    "        \"jfk_coord\": (40.639722, -73.778889),\n",
    "        \"ewr_coord\": (40.6925, -74.168611),\n",
    "        \"lga_coord\": (40.77725, -73.872611),\n",
    "        \"sol_coord\": (40.6892,-74.0445), # Statue of Liberty\n",
    "        \"nyc_coord\": (40.7141667,-74.0063889) \n",
    "    }\n",
    "    \n",
    "    trans_primitives = [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"hour\", \"is_weekend\", \"is_working_hours\", \"part_of_day\"]\n",
    "    trans_primitives += [\"cityblock_distance\", Bearing,\n",
    "                         IsInGeoBox((40.62, -73.85), (40.70, -73.75)),\n",
    "                         IsInGeoBox((40.70, -73.97), (40.77, -73.9))]\n",
    "    trans_primitives += [DistanceToLocation(x) for n, x in coordination_dicts.items()]\n",
    "\n",
    "    # calculate feature_matrix using deep feature synthesis\n",
    "    \n",
    "    ret = ft.dfs(entityset=es,\n",
    "                      target_dataframe_name=\"trips\",\n",
    "                      trans_primitives=trans_primitives,\n",
    "                      verbose=True,\n",
    "                      cutoff_time=cutoff_time,\n",
    "                      approximate='36d',\n",
    "                      max_depth=3,\n",
    "                      max_features=40, \n",
    "                      features_only = features_only)\n",
    "    if features_only:\n",
    "        features = ret\n",
    "    else:\n",
    "        features = ret[1]\n",
    "        df = ret[0]\n",
    "        #df_encoded, features_encoded = ft.encode_features(df, features)\n",
    "    \n",
    "    return df, es, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39967d6",
   "metadata": {},
   "source": [
    "# Spark Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16344aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as spk_func\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import os\n",
    "import re\n",
    "\n",
    "def udf_schema_from_pandas(df):\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    pandas_udf_type = []\n",
    "    for name, t in zip(df.dtypes.index.to_list(), df.dtypes.to_list()):\n",
    "        t = t.name.replace('object', 'string').replace('float64', 'double').replace('Int64', 'int').replace(\"category\", \"int\")\n",
    "        pandas_udf_type.append(f\"{name} {t}\")\n",
    "    print(f\"total num feature is {len(pandas_udf_type)}\")\n",
    "    return \",\".join(pandas_udf_type)\n",
    "    \n",
    "def spark_modelling_features(df, feature_list = None, features_only = False):\n",
    "    hname = os.uname()[1]\n",
    "    # connect to spark\n",
    "    spark = SparkSession.builder.master(f'spark://{hname}:7077')\\\n",
    "            .appName(\"featuretools_pandas_udf\").getOrCreate()\n",
    "    \n",
    "    # create spark dataframe\n",
    "    sparkDF = spark.createDataFrame(df)\n",
    "    #sparkDF.show()\n",
    "    \n",
    "    if feature_list:\n",
    "        top_features = ft.load_features(feature_list)        \n",
    "    else:\n",
    "        top_features = None\n",
    "        \n",
    "    tmp_df, _, _ = modelling_features(df.head(100), feature_list = top_features, features_only = features_only)\n",
    "    pandas_udf_type = udf_schema_from_pandas(tmp_df)\n",
    "        \n",
    "    @pandas_udf(pandas_udf_type)\n",
    "    def pandas_udf_ft(df):\n",
    "        print(df.head())\n",
    "        df, _, _ = modelling_features(df, feature_list = top_features, features_only = features_only)\n",
    "        df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        print(df.head())\n",
    "        return df \n",
    "\n",
    "    # execution\n",
    "    sparkDF = sparkDF.withColumn(\"all_in_one\", spk_func.struct(*[spk_func.col(x) for x in sparkDF.columns]))\n",
    "    sparkDF = sparkDF.select(pandas_udf_ft(\"all_in_one\"))\n",
    "    sparkDF = sparkDF.drop(\"all_in_one\")\n",
    "    try:\n",
    "        df = sparkDF.toPandas()\n",
    "    except:\n",
    "        spark.stop()\n",
    "    spark.stop()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab2b9f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train full took 0.14271353837102652 sec\n",
      "Data Wrangling for train took 0.01885065296664834 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount        float64\n",
      "pickup_datetime     object\n",
      "passenger_count      int64\n",
      "pickup_latlong      object\n",
      "dropoff_latlong     object\n",
      "dtype: object\n",
      "Built 27 features\n",
      "Elapsed: 00:01 | Progress: 100%|██████████\n",
      "total num feature is 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5000</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.8443</td>\n",
       "      <td>40.7213</td>\n",
       "      <td>-73.8416</td>\n",
       "      <td>40.7123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9000</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.0160</td>\n",
       "      <td>40.7113</td>\n",
       "      <td>-73.9793</td>\n",
       "      <td>40.7820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7000</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.9827</td>\n",
       "      <td>40.7613</td>\n",
       "      <td>-73.9912</td>\n",
       "      <td>40.7506</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7000</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.9871</td>\n",
       "      <td>40.7331</td>\n",
       "      <td>-73.9916</td>\n",
       "      <td>40.7581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3000</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.9681</td>\n",
       "      <td>40.7680</td>\n",
       "      <td>-73.9567</td>\n",
       "      <td>40.7838</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>9.0000</td>\n",
       "      <td>2013-09-24 07:39:00 UTC</td>\n",
       "      <td>-73.9480</td>\n",
       "      <td>40.7848</td>\n",
       "      <td>-73.9643</td>\n",
       "      <td>40.7923</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>6.0000</td>\n",
       "      <td>2014-05-15 12:15:45 UTC</td>\n",
       "      <td>-73.9629</td>\n",
       "      <td>40.7991</td>\n",
       "      <td>-73.9742</td>\n",
       "      <td>40.7865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>2015-02-19 17:40:43 UTC</td>\n",
       "      <td>-73.9968</td>\n",
       "      <td>40.7235</td>\n",
       "      <td>-73.9920</td>\n",
       "      <td>40.7247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>6.9000</td>\n",
       "      <td>2009-10-10 23:35:00 UTC</td>\n",
       "      <td>-73.9837</td>\n",
       "      <td>40.7567</td>\n",
       "      <td>-73.9827</td>\n",
       "      <td>40.7671</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>5.7000</td>\n",
       "      <td>2010-11-09 16:09:00 UTC</td>\n",
       "      <td>-73.9757</td>\n",
       "      <td>40.7917</td>\n",
       "      <td>-73.9823</td>\n",
       "      <td>40.7750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97983 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "0           4.5000  2009-06-15 17:26:21 UTC          -73.8443   \n",
       "1          16.9000  2010-01-05 16:52:16 UTC          -74.0160   \n",
       "2           5.7000  2011-08-18 00:35:00 UTC          -73.9827   \n",
       "3           7.7000  2012-04-21 04:30:42 UTC          -73.9871   \n",
       "4           5.3000  2010-03-09 07:51:00 UTC          -73.9681   \n",
       "...            ...                      ...               ...   \n",
       "99995       9.0000  2013-09-24 07:39:00 UTC          -73.9480   \n",
       "99996       6.0000  2014-05-15 12:15:45 UTC          -73.9629   \n",
       "99997       5.0000  2015-02-19 17:40:43 UTC          -73.9968   \n",
       "99998       6.9000  2009-10-10 23:35:00 UTC          -73.9837   \n",
       "99999       5.7000  2010-11-09 16:09:00 UTC          -73.9757   \n",
       "\n",
       "       pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0              40.7213           -73.8416           40.7123                1  \n",
       "1              40.7113           -73.9793           40.7820                1  \n",
       "2              40.7613           -73.9912           40.7506                2  \n",
       "3              40.7331           -73.9916           40.7581                1  \n",
       "4              40.7680           -73.9567           40.7838                1  \n",
       "...                ...                ...               ...              ...  \n",
       "99995          40.7848           -73.9643           40.7923                5  \n",
       "99996          40.7991           -73.9742           40.7865                1  \n",
       "99997          40.7235           -73.9920           40.7247                1  \n",
       "99998          40.7567           -73.9827           40.7671                4  \n",
       "99999          40.7917           -73.9823           40.7750                1  \n",
       "\n",
       "[97983 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small data to test\n",
    "from utils import Timer\n",
    "from IPython.display import display\n",
    "\n",
    "path = \"../data/\"\n",
    "TRAIN_PATH = f'{path}/train.csv'\n",
    "TEST_PATH = f'{path}/test.csv'\n",
    "\n",
    "cols = [\n",
    "    'fare_amount', 'pickup_datetime','pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'passenger_count'\n",
    "]\n",
    " \n",
    "#sampled_line = 100000\n",
    "with Timer(f\"Load train full\"):\n",
    "    train = pd.read_csv(TRAIN_PATH, usecols=cols, nrows=100000)\n",
    "\n",
    "with Timer(\"Data Wrangling for train\"):\n",
    "    train = clean_df(train)\n",
    "\n",
    "top_features = ft.load_features(\"featuretools_humanknowledge_nyc_taxi_top_features.txt\")\n",
    "train = spark_modelling_features(train)\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845b6f0",
   "metadata": {},
   "source": [
    "# EvalML Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"featuretools_process_nyc_taxi.parquet\")\n",
    "train = train.head(10000000)\n",
    "\n",
    "def get_split_sets(train):\n",
    "    x = train.drop(columns=['fare_amount'])\n",
    "    y = train['fare_amount'].values\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=123)\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "with Timer(\"split train and val\"):\n",
    "    x_train, x_val, y_train, y_val = get_split_sets(train)\n",
    "    \n",
    "# looking for right ml pipeline\n",
    "import evalml\n",
    "from evalml import AutoMLSearch\n",
    "\n",
    "automl = AutoMLSearch(X_train=x_train,\n",
    "                      y_train=y_train,\n",
    "                      X_holdout=X_val,\n",
    "                      y_holdout=y_val,\n",
    "                      problem_type=\"regression\",\n",
    "                      objective=\"root mean squared error\",\n",
    "                      verbose=True,)\n",
    "automl.search()\n",
    "\n",
    "best_pipeline = automl.best_pipeline\n",
    "with Timer(\"train\"):\n",
    "    best_pipeline.fit(x_train, y_train)\n",
    "    \n",
    "best_pipeline.score(X_val, y_val, objectives=[\"root mean squared error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a87d93",
   "metadata": {},
   "source": [
    "# LGBM Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f2b6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount                                   float64\n",
      "passenger_count                                 int64\n",
      "GEOBOXdropoff_latlong406273854077375             bool\n",
      "GEOBOXpickup_latlong406273854077375              bool\n",
      "GEOBOXdropoff_latlong40773974077739              bool\n",
      "GEOBOXpickup_latlong40773974077739               bool\n",
      "BEARINGdropoff_latlongpickup_latlong          float64\n",
      "CITYBLOCKdropoff_latlongpickup_latlong        float64\n",
      "IS_NIGHT_HOURpickup_datetime                     bool\n",
      "IS_NOON_HOURpickup_datetime                      bool\n",
      "IS_RUSH_HOURpickup_datetime                      bool\n",
      "passenger_cntIS_NIGHT_HOURfirst_trips_time       bool\n",
      "passenger_cntIS_NOON_HOURfirst_trips_time        bool\n",
      "passenger_cntIS_RUSH_HOURfirst_trips_time        bool\n",
      "dtype: object\n",
      "split train and val took 15.84280596114695 sec\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.153722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10023\n",
      "[LightGBM] [Info] Number of data points in the train set: 48884359, number of used features: 11\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Info] Start training from score 11.323864\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 4.50114\n",
      "[200]\tvalid_0's rmse: 4.42636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's rmse: 4.40651\n",
      "[400]\tvalid_0's rmse: 4.39837\n",
      "[500]\tvalid_0's rmse: 4.39209\n",
      "[600]\tvalid_0's rmse: 4.38852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[700]\tvalid_0's rmse: 4.38532\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[800]\tvalid_0's rmse: 4.38198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[900]\tvalid_0's rmse: 4.37956\n",
      "[1000]\tvalid_0's rmse: 4.37783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 4.37783\n",
      "train took 863.6102110962383 sec\n",
      "predict took 111.67963516479358 sec\n",
      "calculate rmse took 0.054500825237482786 sec\n",
      "LightGBM RMSE 4.377826140151559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train = pd.read_parquet(\"featuretools_process_nyc_taxi.parquet\")\n",
    "train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "print(train.dtypes)\n",
    "\n",
    "def get_split_sets(train):\n",
    "    x = train.drop(columns=['fare_amount'])\n",
    "    y = train['fare_amount'].values\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=0)\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "with Timer(\"split train and val\"):\n",
    "    x_train, x_val, y_train, y_val = get_split_sets(train)\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 4,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'max_bin' : 5000 ,\n",
    "        'bagging_freq': 20,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_child_samples': 10,\n",
    "        'scale_pos_weight':1,\n",
    "        'zero_as_missing': True,\n",
    "        'seed':0,\n",
    "        'num_rounds':1000,\n",
    "        'num_boost_round': 10000,\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "\n",
    "\n",
    "# lgbm_train = lgbm.Dataset(x_train, y_train, silent=False, categorical_feature=['passenger_count','year','time','dayofyear','weekday'])\n",
    "# lgbm_val = lgbm.Dataset(x_val, y_val, silent=False, categorical_feature=['passenger_count','year','time','dayofyear','weekday'])\n",
    "\n",
    "lgbm_train = lgbm.Dataset(x_train, y_train, silent=False)\n",
    "lgbm_val = lgbm.Dataset(x_val, y_val, silent=False)\n",
    "\n",
    "\n",
    "with Timer(\"train\"):\n",
    "    model = lgbm.train(params=params, train_set=lgbm_train, valid_sets=lgbm_val, verbose_eval=100)\n",
    "    \n",
    "with Timer(\"predict\"):\n",
    "    pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "with Timer(\"calculate rmse\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "print('LightGBM RMSE', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2b7ea",
   "metadata": {},
   "source": [
    "# LGBM for 10 million sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d608d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split train and val took 3.7570005068555474 sec\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 30536\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 3.177760\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 4.25898\n",
      "[200]\tvalid_0's rmse: 4.12228\n",
      "[300]\tvalid_0's rmse: 4.08928\n",
      "[400]\tvalid_0's rmse: 4.0765\n",
      "[500]\tvalid_0's rmse: 4.07053\n",
      "[600]\tvalid_0's rmse: 4.06616\n",
      "[700]\tvalid_0's rmse: 4.06431\n",
      "[800]\tvalid_0's rmse: 4.06246\n",
      "[900]\tvalid_0's rmse: 4.06072\n",
      "[1000]\tvalid_0's rmse: 4.05996\n",
      "[1100]\tvalid_0's rmse: 4.05938\n",
      "Early stopping, best iteration is:\n",
      "[1107]\tvalid_0's rmse: 4.05933\n",
      "train took 803.9622147129849 sec\n",
      "predict took 4.348827651701868 sec\n",
      "calculate rmse took 0.018370551988482475 sec\n",
      "LightGBM RMSE 4.059332078403533\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"split train and val\"):\n",
    "    x_train, x_val, y_train, y_val = get_split_sets(train)\n",
    "\n",
    "lgbm_params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting': 'gbdt',\n",
    "    'reg_sqrt': True,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 1200,\n",
    "    'max_depth': -1,\n",
    "    'max_bin': 5000,\n",
    "    'num_rounds': 1200,\n",
    "    'early_stopping_round': 50,\n",
    "    'metric': 'rmse'\n",
    "}\n",
    "\n",
    "lgbm_train = lgbm.Dataset(x_train, y_train, silent=False)\n",
    "lgbm_val = lgbm.Dataset(x_val, y_val, silent=False)\n",
    "\n",
    "with Timer(\"train\"):\n",
    "    model = lgbm.train(params=lgbm_params, train_set=lgbm_train, valid_sets=lgbm_val, verbose_eval=100)\n",
    "    \n",
    "with Timer(\"predict\"):\n",
    "    pred = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "with Timer(\"calculate rmse\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "\n",
    "print('LightGBM RMSE', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b218373",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d08d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_features(df):\n",
    "    \"\"\"adds features for use in the EDA section\"\"\"\n",
    "    df = shared_features(df)\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            hour=df.pickup_datetime.dt.hour,\n",
    "            close_to_airport='No',\n",
    "            fare_per_km=df.fare_amount*1000/df.distance,\n",
    "            direction_bucket = pd.cut(df.direction, np.linspace(-180, 180, 37)),\n",
    "\n",
    "            #small location buckets\n",
    "            pickup_long_bucket=pd.cut(df.pickup_longitude, bins=2550, labels=False),\n",
    "            pickup_lat_bucket=pd.cut(df.pickup_latitude, bins=2200, labels=False),\n",
    "            dropoff_long_bucket=pd.cut(df.dropoff_longitude, bins=2550, labels=False),\n",
    "            dropoff_lat_bucket=pd.cut(df.dropoff_latitude, bins=2200, labels=False),\n",
    "\n",
    "\n",
    "            #large location buckets\n",
    "            pickup_long_bucket_big=pd.cut(df.pickup_longitude, bins=255, labels=False),\n",
    "            pickup_lat_bucket_big=pd.cut(df.pickup_latitude, bins=220, labels=False),\n",
    "            dropoff_long_bucket_big=pd.cut(df.dropoff_longitude, bins=255, labels=False),\n",
    "            dropoff_lat_bucket_big=pd.cut(df.dropoff_latitude, bins=220, labels=False)\n",
    "        )\n",
    "        .drop(columns='pickup_datetime')\n",
    "        .query(\"0 < distance\")\n",
    "    )\n",
    "\n",
    "    df.loc[((df['pickup_dist_jfk']<1500) | (df['dropoff_dist_jfk']<1500)), 'close_to_airport'] = 'JFK'\n",
    "    df.loc[((df['pickup_dist_lga']<1500) | (df['dropoff_dist_lga']<1500)), 'close_to_airport'] = 'LaGuardia'\n",
    "    df.loc[((df['pickup_dist_nla']<1500) | (df['dropoff_dist_nla']<1500)), 'close_to_airport'] = 'Newark'  \n",
    "    return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
