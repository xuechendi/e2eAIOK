{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c7f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train data took 89.32287148898467 sec\n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0         -1.288826         0.710721          -1.288779          0.710563   \n",
      "1         -1.291824         0.710546          -1.291182          0.711780   \n",
      "2         -1.291242         0.711418          -1.291391          0.711231   \n",
      "3         -1.291319         0.710927          -1.291396          0.711363   \n",
      "4         -1.290987         0.711536          -1.290787          0.711811   \n",
      "\n",
      "   passenger_count  hour  day  month  weekday  year   jfk_dist   ewr_dist  \\\n",
      "0                1    17   15      6        0  2009  20.265840  55.176046   \n",
      "1                1    16    5      1        1  2010  44.667679  31.832358   \n",
      "2                2     0   18      8        3  2011  43.597686  33.712082   \n",
      "3                1     4   21      4        5  2012  42.642965  32.556289   \n",
      "4                1     7    9      3        1  2010  43.329953  39.406828   \n",
      "\n",
      "    lga_dist   sol_dist   nyc_dist  distance   bearing  \n",
      "0  14.342611  34.543548  27.572573  1.030764 -2.918897  \n",
      "1  23.130775  15.125872   8.755732  8.450134 -0.375217  \n",
      "2  19.865289  17.722624   9.847344  1.389525  2.599961  \n",
      "3  21.063132  15.738963   7.703421  2.799270  0.133905  \n",
      "4  15.219339  23.732406  15.600745  1.999157 -0.502703  \n",
      "Train data wrangling took 237.8626885036938 sec\n",
      "Train/Valid split took 33.587898958008736 sec\n",
      "Prepare DataLoader took 4.557520151138306e-05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:177: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1491: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via 'params' instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.625020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 55093\n",
      "[LightGBM] [Info] Number of data points in the train set: 48884102, number of used features: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=0.8 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Info] Start training from score 11.323812\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's rmse: 3.63438\n",
      "[1000]\tvalid_0's rmse: 3.57345\n",
      "[1500]\tvalid_0's rmse: 3.54202\n",
      "[2000]\tvalid_0's rmse: 3.52315\n",
      "[2500]\tvalid_0's rmse: 3.51118\n",
      "[3000]\tvalid_0's rmse: 3.50184\n",
      "[3500]\tvalid_0's rmse: 3.4938\n",
      "[4000]\tvalid_0's rmse: 3.48643\n",
      "[4500]\tvalid_0's rmse: 3.48115\n",
      "[5000]\tvalid_0's rmse: 3.47663\n",
      "[5500]\tvalid_0's rmse: 3.47138\n",
      "[6000]\tvalid_0's rmse: 3.4681\n",
      "[6500]\tvalid_0's rmse: 3.46498\n",
      "[7000]\tvalid_0's rmse: 3.46262\n",
      "[7500]\tvalid_0's rmse: 3.45953\n",
      "[8000]\tvalid_0's rmse: 3.45725\n",
      "[8500]\tvalid_0's rmse: 3.4553\n",
      "[9000]\tvalid_0's rmse: 3.45308\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %% [code]\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "import os\n",
    "import gc\n",
    "from utils import Timer\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    return df[(df.fare_amount > 0)  & (df.fare_amount <= 500) &\n",
    "          # (df.passenger_count >= 0) & (df.passenger_count <= 8)  &\n",
    "           ((df.pickup_longitude != 0) & (df.pickup_latitude != 0) & (df.dropoff_longitude != 0) & (df.dropoff_latitude != 0) )]\n",
    "\n",
    "\n",
    "# To Compute Haversine distance\n",
    "def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between pickup and dropoff coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n",
    "                                                             [pickup_lat, pickup_lon, \n",
    "                                                              dropoff_lat, dropoff_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = dropoff_lat - pickup_lat\n",
    "    dlon = dropoff_lon - pickup_lon\n",
    "    \n",
    "    #Compute haversine distance\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n",
    "    return 2 * R_earth * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    \n",
    "def sphere_dist_bear(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between pickup and dropoff coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n",
    "                                                             [pickup_lat, pickup_lon, \n",
    "                                                              dropoff_lat, dropoff_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = dropoff_lat - pickup_lat\n",
    "    dlon = pickup_lon - dropoff_lon\n",
    "    \n",
    "    #Compute bearing distance\n",
    "    a = np.arctan2(np.sin(dlon * np.cos(dropoff_lat)),np.cos(pickup_lat) * np.sin(dropoff_lat) - np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon))\n",
    "    return a\n",
    "\n",
    "def radian_conv(degree):\n",
    "    \"\"\"\n",
    "    Return radian.\n",
    "    \"\"\"\n",
    "    return  np.radians(degree)    \n",
    "\n",
    "def add_airport_dist(dataset):\n",
    "    \"\"\"\n",
    "    Return minumum distance from pickup or dropoff coordinates to each airport.\n",
    "    JFK: John F. Kennedy International Airport\n",
    "    EWR: Newark Liberty International Airport\n",
    "    LGA: LaGuardia Airport\n",
    "    SOL: Statue of Liberty \n",
    "    NYC: Newyork Central\n",
    "    \"\"\"\n",
    "    jfk_coord = (40.639722, -73.778889)\n",
    "    ewr_coord = (40.6925, -74.168611)\n",
    "    lga_coord = (40.77725, -73.872611)\n",
    "    sol_coord = (40.6892,-74.0445) # Statue of Liberty\n",
    "    nyc_coord = (40.7141667,-74.0063889) \n",
    "     \n",
    "    pickup_lat = dataset['pickup_latitude']\n",
    "    dropoff_lat = dataset['dropoff_latitude']\n",
    "    pickup_lon = dataset['pickup_longitude']\n",
    "    dropoff_lon = dataset['dropoff_longitude']\n",
    "    \n",
    "    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n",
    "    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n",
    "    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n",
    "    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n",
    "    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n",
    "    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)\n",
    "    pickup_sol = sphere_dist(pickup_lat, pickup_lon, sol_coord[0], sol_coord[1]) \n",
    "    dropoff_sol = sphere_dist(sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon)\n",
    "    pickup_nyc = sphere_dist(pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1]) \n",
    "    dropoff_nyc = sphere_dist(nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon)\n",
    "    \n",
    "    dataset['jfk_dist'] = pickup_jfk + dropoff_jfk\n",
    "    dataset['ewr_dist'] = pickup_ewr + dropoff_ewr\n",
    "    dataset['lga_dist'] = pickup_lga + dropoff_lga\n",
    "    dataset['sol_dist'] = pickup_sol + dropoff_sol\n",
    "    dataset['nyc_dist'] = pickup_nyc + dropoff_nyc\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def add_datetime_info(dataset):\n",
    "    #Convert to datetime format\n",
    "    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    \n",
    "    dataset['hour'] = dataset.pickup_datetime.dt.hour\n",
    "    dataset['day'] = dataset.pickup_datetime.dt.day\n",
    "    dataset['month'] = dataset.pickup_datetime.dt.month\n",
    "    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n",
    "    dataset['year'] = dataset.pickup_datetime.dt.year\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# === main ===\n",
    "\n",
    "path = \"../data/\"\n",
    "with Timer(\"read train data\"):\n",
    "    # Reading Data\n",
    "    train_df =  pd.read_csv(f'{path}/train.csv')\n",
    "    #train_df.sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "with Timer(\"Train data wrangling\"):    \n",
    "    #Drop rows with null values\n",
    "    train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "    train_df = clean_df(train_df)\n",
    "    train_df = add_datetime_info(train_df)\n",
    "    train_df = add_airport_dist(train_df)\n",
    "    train_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n",
    "                                       train_df['dropoff_latitude'] , train_df['dropoff_longitude']) \n",
    "\n",
    "    train_df['bearing'] = sphere_dist_bear(train_df['pickup_latitude'], train_df['pickup_longitude'], \n",
    "                                       train_df['dropoff_latitude'] , train_df['dropoff_longitude'])                                    \n",
    "    train_df['pickup_latitude'] = radian_conv(train_df['pickup_latitude'])\n",
    "    train_df['pickup_longitude'] = radian_conv(train_df['pickup_longitude'])\n",
    "    train_df['dropoff_latitude'] = radian_conv(train_df['dropoff_latitude'])\n",
    "    train_df['dropoff_longitude'] = radian_conv(train_df['dropoff_longitude'])\n",
    "    train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\n",
    "\n",
    "    y = train_df['fare_amount']\n",
    "    train_df = train_df.drop(columns=['fare_amount'])\n",
    "    print(train_df.head())\n",
    "\n",
    "with Timer(\"Train/Valid split\"):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(train_df,y,random_state=123,test_size=0.10)\n",
    "\n",
    "del train_df\n",
    "del y\n",
    "gc.collect()\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 4,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'max_bin' : 5000 ,\n",
    "        'bagging_freq': 20,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_child_samples': 10,\n",
    "        'scale_pos_weight':1,\n",
    "        'zero_as_missing': True,\n",
    "        'seed':0,\n",
    "        'num_rounds':50000\n",
    "    }\n",
    "\n",
    "with Timer(\"Prepare DataLoader\"):\n",
    "    train_set = lgbm.Dataset(x_train, y_train, silent=False,categorical_feature=['year','month','day','weekday'])\n",
    "    valid_set = lgbm.Dataset(x_test, y_test, silent=False,categorical_feature=['year','month','day','weekday'])\n",
    "    \n",
    "with Timer(\"train lgbm model\"):\n",
    "    model = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=500,verbose_eval=500, valid_sets=valid_set)\n",
    "\n",
    "del x_train\n",
    "del y_train\n",
    "del x_test\n",
    "del y_test\n",
    "gc.collect()\n",
    "   \n",
    "with Timer(\"read test data\"):\n",
    "    test_df =  pd.read_csv(f'{path}/test.csv')\n",
    "print(test_df.head())\n",
    "\n",
    "with Timer(\"test data wrangling\"):\n",
    "    test_df = add_datetime_info(test_df)\n",
    "    test_df = add_airport_dist(test_df)\n",
    "    test_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n",
    "                                       test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\n",
    "\n",
    "    test_df['bearing'] = sphere_dist_bear(test_df['pickup_latitude'], test_df['pickup_longitude'], \n",
    "                                        test_df['dropoff_latitude'] , test_df['dropoff_longitude'])  \n",
    "    test_df['pickup_latitude'] = radian_conv(test_df['pickup_latitude'])\n",
    "    test_df['pickup_longitude'] = radian_conv(test_df['pickup_longitude'])\n",
    "    test_df['dropoff_latitude'] = radian_conv(test_df['dropoff_latitude'])\n",
    "    test_df['dropoff_longitude'] = radian_conv(test_df['dropoff_longitude'])\n",
    "\n",
    "    test_key = test_df['key']\n",
    "    test_df = test_df.drop(columns=['key', 'pickup_datetime'])\n",
    "\n",
    "with Timer(\"predict lgbm model\"):\n",
    "    #Predict from test set\n",
    "    prediction = model.predict(test_df, num_iteration = model.best_iteration)\n",
    "    \n",
    "with Timer(\"save prediction\"):\n",
    "    submission = pd.DataFrame({\n",
    "            \"key\": test_key,\n",
    "            \"fare_amount\": prediction\n",
    "    })\n",
    "\n",
    "    submission.to_csv(f'{path}/taxi_fare_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %% [code]\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "import os\n",
    "import gc\n",
    "from utils import Timer\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    return df[(df.fare_amount > 0)  & (df.fare_amount <= 500) &\n",
    "          # (df.passenger_count >= 0) & (df.passenger_count <= 8)  &\n",
    "           ((df.pickup_longitude != 0) & (df.pickup_latitude != 0) & (df.dropoff_longitude != 0) & (df.dropoff_latitude != 0) )]\n",
    "\n",
    "\n",
    "# To Compute Haversine distance\n",
    "def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between pickup and dropoff coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n",
    "                                                             [pickup_lat, pickup_lon, \n",
    "                                                              dropoff_lat, dropoff_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = dropoff_lat - pickup_lat\n",
    "    dlon = dropoff_lon - pickup_lon\n",
    "    \n",
    "    #Compute haversine distance\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n",
    "    return 2 * R_earth * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    \n",
    "def sphere_dist_bear(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n",
    "    \"\"\"\n",
    "    Return distance along great radius between pickup and dropoff coordinates.\n",
    "    \"\"\"\n",
    "    #Define earth radius (km)\n",
    "    R_earth = 6371\n",
    "    #Convert degrees to radians\n",
    "    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n",
    "                                                             [pickup_lat, pickup_lon, \n",
    "                                                              dropoff_lat, dropoff_lon])\n",
    "    #Compute distances along lat, lon dimensions\n",
    "    dlat = dropoff_lat - pickup_lat\n",
    "    dlon = pickup_lon - dropoff_lon\n",
    "    \n",
    "    #Compute bearing distance\n",
    "    a = np.arctan2(np.sin(dlon * np.cos(dropoff_lat)),np.cos(pickup_lat) * np.sin(dropoff_lat) - np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon))\n",
    "    return a\n",
    "\n",
    "def radian_conv(degree):\n",
    "    \"\"\"\n",
    "    Return radian.\n",
    "    \"\"\"\n",
    "    return  np.radians(degree)    \n",
    "\n",
    "def add_airport_dist(dataset):\n",
    "    \"\"\"\n",
    "    Return minumum distance from pickup or dropoff coordinates to each airport.\n",
    "    JFK: John F. Kennedy International Airport\n",
    "    EWR: Newark Liberty International Airport\n",
    "    LGA: LaGuardia Airport\n",
    "    SOL: Statue of Liberty \n",
    "    NYC: Newyork Central\n",
    "    \"\"\"\n",
    "    jfk_coord = (40.639722, -73.778889)\n",
    "    ewr_coord = (40.6925, -74.168611)\n",
    "    lga_coord = (40.77725, -73.872611)\n",
    "    sol_coord = (40.6892,-74.0445) # Statue of Liberty\n",
    "    nyc_coord = (40.7141667,-74.0063889) \n",
    "     \n",
    "    pickup_lat = dataset['pickup_latitude']\n",
    "    dropoff_lat = dataset['dropoff_latitude']\n",
    "    pickup_lon = dataset['pickup_longitude']\n",
    "    dropoff_lon = dataset['dropoff_longitude']\n",
    "    \n",
    "    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n",
    "    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n",
    "    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n",
    "    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n",
    "    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n",
    "    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)\n",
    "    pickup_sol = sphere_dist(pickup_lat, pickup_lon, sol_coord[0], sol_coord[1]) \n",
    "    dropoff_sol = sphere_dist(sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon)\n",
    "    pickup_nyc = sphere_dist(pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1]) \n",
    "    dropoff_nyc = sphere_dist(nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon)\n",
    "    \n",
    "    dataset['jfk_dist'] = pickup_jfk + dropoff_jfk\n",
    "    dataset['ewr_dist'] = pickup_ewr + dropoff_ewr\n",
    "    dataset['lga_dist'] = pickup_lga + dropoff_lga\n",
    "    dataset['sol_dist'] = pickup_sol + dropoff_sol\n",
    "    dataset['nyc_dist'] = pickup_nyc + dropoff_nyc\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def add_datetime_info(dataset):\n",
    "    #Convert to datetime format\n",
    "    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    \n",
    "    dataset['hour'] = dataset.pickup_datetime.dt.hour\n",
    "    dataset['day'] = dataset.pickup_datetime.dt.day\n",
    "    dataset['month'] = dataset.pickup_datetime.dt.month\n",
    "    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n",
    "    dataset['year'] = dataset.pickup_datetime.dt.year\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# === main ===\n",
    "\n",
    "path = \"../data/\"\n",
    "with Timer(\"read train data\"):\n",
    "    # Reading Data\n",
    "    train_df =  pd.read_csv(f'{path}/train.csv')\n",
    "    train_df.sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "with Timer(\"Train data wrangling\"):    \n",
    "    #Drop rows with null values\n",
    "    train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "    train_df = clean_df(train_df)\n",
    "    train_df = add_datetime_info(train_df)\n",
    "    train_df = add_airport_dist(train_df)\n",
    "    train_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n",
    "                                       train_df['dropoff_latitude'] , train_df['dropoff_longitude']) \n",
    "\n",
    "    train_df['bearing'] = sphere_dist_bear(train_df['pickup_latitude'], train_df['pickup_longitude'], \n",
    "                                       train_df['dropoff_latitude'] , train_df['dropoff_longitude'])                                    \n",
    "    train_df['pickup_latitude'] = radian_conv(train_df['pickup_latitude'])\n",
    "    train_df['pickup_longitude'] = radian_conv(train_df['pickup_longitude'])\n",
    "    train_df['dropoff_latitude'] = radian_conv(train_df['dropoff_latitude'])\n",
    "    train_df['dropoff_longitude'] = radian_conv(train_df['dropoff_longitude'])\n",
    "    train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\n",
    "\n",
    "    y = train_df['fare_amount']\n",
    "    train_df = train_df.drop(columns=['fare_amount'])\n",
    "    print(train_df.head())\n",
    "\n",
    "with Timer(\"Train/Valid split\"):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(train_df,y,random_state=123,test_size=0.10)\n",
    "\n",
    "del train_df\n",
    "del y\n",
    "gc.collect()\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 4,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'max_bin' : 5000 ,\n",
    "        'bagging_freq': 20,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_child_samples': 10,\n",
    "        'scale_pos_weight':1,\n",
    "        'zero_as_missing': True,\n",
    "        'seed':0,\n",
    "        'num_rounds':50000\n",
    "    }\n",
    "\n",
    "with Timer(\"Prepare DataLoader\"):\n",
    "    train_set = lgbm.Dataset(x_train, y_train, silent=False,categorical_feature=['year','month','day','weekday'])\n",
    "    valid_set = lgbm.Dataset(x_test, y_test, silent=False,categorical_feature=['year','month','day','weekday'])\n",
    "    \n",
    "with Timer(\"train lgbm model\"):\n",
    "    model = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=500,verbose_eval=500, valid_sets=valid_set)\n",
    "\n",
    "del x_train\n",
    "del y_train\n",
    "del x_test\n",
    "del y_test\n",
    "gc.collect()\n",
    "   \n",
    "with Timer(\"read test data\"):\n",
    "    test_df =  pd.read_csv(f'{path}/test.csv')\n",
    "print(test_df.head())\n",
    "\n",
    "with Timer(\"test data wrangling\"):\n",
    "    test_df = add_datetime_info(test_df)\n",
    "    test_df = add_airport_dist(test_df)\n",
    "    test_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n",
    "                                       test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\n",
    "\n",
    "    test_df['bearing'] = sphere_dist_bear(test_df['pickup_latitude'], test_df['pickup_longitude'], \n",
    "                                        test_df['dropoff_latitude'] , test_df['dropoff_longitude'])  \n",
    "    test_df['pickup_latitude'] = radian_conv(test_df['pickup_latitude'])\n",
    "    test_df['pickup_longitude'] = radian_conv(test_df['pickup_longitude'])\n",
    "    test_df['dropoff_latitude'] = radian_conv(test_df['dropoff_latitude'])\n",
    "    test_df['dropoff_longitude'] = radian_conv(test_df['dropoff_longitude'])\n",
    "\n",
    "    test_key = test_df['key']\n",
    "    test_df = test_df.drop(columns=['key', 'pickup_datetime'])\n",
    "\n",
    "with Timer(\"predict lgbm model\"):\n",
    "    #Predict from test set\n",
    "    prediction = model.predict(test_df, num_iteration = model.best_iteration)\n",
    "    \n",
    "with Timer(\"save prediction\"):\n",
    "    submission = pd.DataFrame({\n",
    "            \"key\": test_key,\n",
    "            \"fare_amount\": prediction\n",
    "    })\n",
    "\n",
    "    submission.to_csv(f'{path}/taxi_fare_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f11b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
